digraph {
	graph [size="62.699999999999996,62.699999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
		2178457392896 [label="
 (100, 100)" fillcolor=darkolivegreen1]
		2178603868944 [label=AddmmBackward]
			2178603868992 -> 2178603868944
		2178457341632 [label="fc.bias
 (100)" fillcolor=lightblue]
			2178457341632 -> 2178603868992
		2178603868992 [label=AccumulateGrad]
			2178603868848 -> 2178603868944
		2178603868848 [label=ViewBackward]
			2178603869088 -> 2178603868848
		2178603869088 [label=AvgPool2DBackward]
			2178603869280 -> 2178603869088
		2178603869280 [label=ReluBackward1]
			2178603869376 -> 2178603869280
		2178603869376 [label=AddBackward0]
			2178603869472 -> 2178603869376
		2178603869472 [label=CudnnBatchNormBackward]
			2178603869616 -> 2178603869472
		2178603869616 [label=CudnnConvolutionBackward]
			2178603869808 -> 2178603869616
		2178603869808 [label=ReluBackward1]
			2178603869952 -> 2178603869808
		2178603869952 [label=CudnnBatchNormBackward]
			2178603870048 -> 2178603869952
		2178603870048 [label=CudnnConvolutionBackward]
			2178603869424 -> 2178603870048
		2178603869424 [label=ReluBackward1]
			2178603870336 -> 2178603869424
		2178603870336 [label=AddBackward0]
			2178603870432 -> 2178603870336
		2178603870432 [label=CudnnBatchNormBackward]
			2178603870576 -> 2178603870432
		2178603870576 [label=CudnnConvolutionBackward]
			2178603870768 -> 2178603870576
		2178603870768 [label=ReluBackward1]
			2178603870912 -> 2178603870768
		2178603870912 [label=CudnnBatchNormBackward]
			2178603871008 -> 2178603870912
		2178603871008 [label=CudnnConvolutionBackward]
			2178603870384 -> 2178603871008
		2178603870384 [label=ReluBackward1]
			2178603871296 -> 2178603870384
		2178603871296 [label=AddBackward0]
			2178603871392 -> 2178603871296
		2178603871392 [label=CudnnBatchNormBackward]
			2178603871536 -> 2178603871392
		2178603871536 [label=CudnnConvolutionBackward]
			2178603871728 -> 2178603871536
		2178603871728 [label=ReluBackward1]
			2178603871872 -> 2178603871728
		2178603871872 [label=CudnnBatchNormBackward]
			2178603871968 -> 2178603871872
		2178603871968 [label=CudnnConvolutionBackward]
			2178603872160 -> 2178603871968
		2178603872160 [label=ReluBackward1]
			2178603872208 -> 2178603872160
		2178603872208 [label=AddBackward0]
			2178457314304 -> 2178603872208
		2178457314304 [label=CudnnBatchNormBackward]
			2178457314496 -> 2178457314304
		2178457314496 [label=CudnnConvolutionBackward]
			2178457314976 -> 2178457314496
		2178457314976 [label=ReluBackward1]
			2178457317328 -> 2178457314976
		2178457317328 [label=CudnnBatchNormBackward]
			2178457314736 -> 2178457317328
		2178457314736 [label=CudnnConvolutionBackward]
			2178457314400 -> 2178457314736
		2178457314400 [label=ReluBackward1]
			2178457436512 -> 2178457314400
		2178457436512 [label=AddBackward0]
			2178457436608 -> 2178457436512
		2178457436608 [label=CudnnBatchNormBackward]
			2178457436752 -> 2178457436608
		2178457436752 [label=CudnnConvolutionBackward]
			2178457436944 -> 2178457436752
		2178457436944 [label=ReluBackward1]
			2178457437088 -> 2178457436944
		2178457437088 [label=CudnnBatchNormBackward]
			2178457437184 -> 2178457437088
		2178457437184 [label=CudnnConvolutionBackward]
			2178457436560 -> 2178457437184
		2178457436560 [label=ReluBackward1]
			2178457437472 -> 2178457436560
		2178457437472 [label=AddBackward0]
			2178457437568 -> 2178457437472
		2178457437568 [label=CudnnBatchNormBackward]
			2178457437712 -> 2178457437568
		2178457437712 [label=CudnnConvolutionBackward]
			2178457437904 -> 2178457437712
		2178457437904 [label=ReluBackward1]
			2178457438048 -> 2178457437904
		2178457438048 [label=CudnnBatchNormBackward]
			2178457438144 -> 2178457438048
		2178457438144 [label=CudnnConvolutionBackward]
			2178457438336 -> 2178457438144
		2178457438336 [label=ReluBackward1]
			2178457438480 -> 2178457438336
		2178457438480 [label=AddBackward0]
			2178457438576 -> 2178457438480
		2178457438576 [label=CudnnBatchNormBackward]
			2178457438720 -> 2178457438576
		2178457438720 [label=CudnnConvolutionBackward]
			2178457438912 -> 2178457438720
		2178457438912 [label=ReluBackward1]
			2178457439056 -> 2178457438912
		2178457439056 [label=CudnnBatchNormBackward]
			2178457439152 -> 2178457439056
		2178457439152 [label=CudnnConvolutionBackward]
			2178457438528 -> 2178457439152
		2178457438528 [label=ReluBackward1]
			2178457439440 -> 2178457438528
		2178457439440 [label=AddBackward0]
			2178457439536 -> 2178457439440
		2178457439536 [label=CudnnBatchNormBackward]
			2178457439680 -> 2178457439536
		2178457439680 [label=CudnnConvolutionBackward]
			2178457439872 -> 2178457439680
		2178457439872 [label=ReluBackward1]
			2178457440016 -> 2178457439872
		2178457440016 [label=CudnnBatchNormBackward]
			2178457440112 -> 2178457440016
		2178457440112 [label=CudnnConvolutionBackward]
			2178457439488 -> 2178457440112
		2178457439488 [label=ReluBackward1]
			2178457428176 -> 2178457439488
		2178457428176 [label=AddBackward0]
			2178457428272 -> 2178457428176
		2178457428272 [label=CudnnBatchNormBackward]
			2178457428416 -> 2178457428272
		2178457428416 [label=CudnnConvolutionBackward]
			2178457428608 -> 2178457428416
		2178457428608 [label=ReluBackward1]
			2178457428752 -> 2178457428608
		2178457428752 [label=CudnnBatchNormBackward]
			2178457428848 -> 2178457428752
		2178457428848 [label=CudnnConvolutionBackward]
			2178457428224 -> 2178457428848
		2178457428224 [label=ReluBackward1]
			2178457429136 -> 2178457428224
		2178457429136 [label=CudnnBatchNormBackward]
			2178457429232 -> 2178457429136
		2178457429232 [label=CudnnConvolutionBackward]
			2178457429424 -> 2178457429232
		2178457468608 [label="conv.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
			2178457468608 -> 2178457429424
		2178457429424 [label=AccumulateGrad]
			2178457429184 -> 2178457429136
		2178457468672 [label="bn.weight
 (16)" fillcolor=lightblue]
			2178457468672 -> 2178457429184
		2178457429184 [label=AccumulateGrad]
			2178457428944 -> 2178457429136
		2178457468352 [label="bn.bias
 (16)" fillcolor=lightblue]
			2178457468352 -> 2178457428944
		2178457428944 [label=AccumulateGrad]
			2178457429040 -> 2178457428848
		2178457467584 [label="layer1.0.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			2178457467584 -> 2178457429040
		2178457429040 [label=AccumulateGrad]
			2178457428800 -> 2178457428752
		2178457467648 [label="layer1.0.bn1.weight
 (16)" fillcolor=lightblue]
			2178457467648 -> 2178457428800
		2178457428800 [label=AccumulateGrad]
			2178457428656 -> 2178457428752
		2178457467328 [label="layer1.0.bn1.bias
 (16)" fillcolor=lightblue]
			2178457467328 -> 2178457428656
		2178457428656 [label=AccumulateGrad]
			2178457428560 -> 2178457428416
		2178457466112 [label="layer1.0.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			2178457466112 -> 2178457428560
		2178457428560 [label=AccumulateGrad]
			2178457428368 -> 2178457428272
		2178457467264 [label="layer1.0.bn2.weight
 (16)" fillcolor=lightblue]
			2178457467264 -> 2178457428368
		2178457428368 [label=AccumulateGrad]
			2178457428320 -> 2178457428272
		2178457465472 [label="layer1.0.bn2.bias
 (16)" fillcolor=lightblue]
			2178457465472 -> 2178457428320
		2178457428320 [label=AccumulateGrad]
			2178457428224 -> 2178457428176
			2178457440208 -> 2178457440112
		2178457460032 [label="layer1.1.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			2178457460032 -> 2178457440208
		2178457440208 [label=AccumulateGrad]
			2178457440064 -> 2178457440016
		2178457460160 [label="layer1.1.bn1.weight
 (16)" fillcolor=lightblue]
			2178457460160 -> 2178457440064
		2178457440064 [label=AccumulateGrad]
			2178457439920 -> 2178457440016
		2178457459520 [label="layer1.1.bn1.bias
 (16)" fillcolor=lightblue]
			2178457459520 -> 2178457439920
		2178457439920 [label=AccumulateGrad]
			2178457439824 -> 2178457439680
		2178457458112 [label="layer1.1.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			2178457458112 -> 2178457439824
		2178457439824 [label=AccumulateGrad]
			2178457439632 -> 2178457439536
		2178457459392 [label="layer1.1.bn2.weight
 (16)" fillcolor=lightblue]
			2178457459392 -> 2178457439632
		2178457439632 [label=AccumulateGrad]
			2178457439584 -> 2178457439536
		2178457457472 [label="layer1.1.bn2.bias
 (16)" fillcolor=lightblue]
			2178457457472 -> 2178457439584
		2178457439584 [label=AccumulateGrad]
			2178457439488 -> 2178457439440
			2178457439344 -> 2178457439152
		2178456107968 [label="layer1.2.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			2178456107968 -> 2178457439344
		2178457439344 [label=AccumulateGrad]
			2178457439104 -> 2178457439056
		2178456106752 [label="layer1.2.bn1.weight
 (16)" fillcolor=lightblue]
			2178456106752 -> 2178457439104
		2178457439104 [label=AccumulateGrad]
			2178457438960 -> 2178457439056
		2178456107776 [label="layer1.2.bn1.bias
 (16)" fillcolor=lightblue]
			2178456107776 -> 2178457438960
		2178457438960 [label=AccumulateGrad]
			2178457438864 -> 2178457438720
		2178457279424 [label="layer1.2.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			2178457279424 -> 2178457438864
		2178457438864 [label=AccumulateGrad]
			2178457438672 -> 2178457438576
		2178457279168 [label="layer1.2.bn2.weight
 (16)" fillcolor=lightblue]
			2178457279168 -> 2178457438672
		2178457438672 [label=AccumulateGrad]
			2178457438624 -> 2178457438576
		2178457278144 [label="layer1.2.bn2.bias
 (16)" fillcolor=lightblue]
			2178457278144 -> 2178457438624
		2178457438624 [label=AccumulateGrad]
			2178457438528 -> 2178457438480
			2178457438288 -> 2178457438144
		2178457277184 [label="layer2.0.conv1.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
			2178457277184 -> 2178457438288
		2178457438288 [label=AccumulateGrad]
			2178457438096 -> 2178457438048
		2178457277632 [label="layer2.0.bn1.weight
 (32)" fillcolor=lightblue]
			2178457277632 -> 2178457438096
		2178457438096 [label=AccumulateGrad]
			2178457437952 -> 2178457438048
		2178457276608 [label="layer2.0.bn1.bias
 (32)" fillcolor=lightblue]
			2178457276608 -> 2178457437952
		2178457437952 [label=AccumulateGrad]
			2178457437856 -> 2178457437712
		2178457279296 [label="layer2.0.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			2178457279296 -> 2178457437856
		2178457437856 [label=AccumulateGrad]
			2178457437664 -> 2178457437568
		2178457276544 [label="layer2.0.bn2.weight
 (32)" fillcolor=lightblue]
			2178457276544 -> 2178457437664
		2178457437664 [label=AccumulateGrad]
			2178457437616 -> 2178457437568
		2178457279808 [label="layer2.0.bn2.bias
 (32)" fillcolor=lightblue]
			2178457279808 -> 2178457437616
		2178457437616 [label=AccumulateGrad]
			2178457437520 -> 2178457437472
		2178457437520 [label=CudnnBatchNormBackward]
			2178457438240 -> 2178457437520
		2178457438240 [label=CudnnConvolutionBackward]
			2178457438336 -> 2178457438240
			2178457438384 -> 2178457438240
		2178457279232 [label="layer2.0.downsample.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
			2178457279232 -> 2178457438384
		2178457438384 [label=AccumulateGrad]
			2178457437808 -> 2178457437520
		2178457279360 [label="layer2.0.downsample.1.weight
 (32)" fillcolor=lightblue]
			2178457279360 -> 2178457437808
		2178457437808 [label=AccumulateGrad]
			2178457437760 -> 2178457437520
		2178457278848 [label="layer2.0.downsample.1.bias
 (32)" fillcolor=lightblue]
			2178457278848 -> 2178457437760
		2178457437760 [label=AccumulateGrad]
			2178457437376 -> 2178457437184
		2178457427328 [label="layer2.1.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			2178457427328 -> 2178457437376
		2178457437376 [label=AccumulateGrad]
			2178457437136 -> 2178457437088
		2178457427456 [label="layer2.1.bn1.weight
 (32)" fillcolor=lightblue]
			2178457427456 -> 2178457437136
		2178457437136 [label=AccumulateGrad]
			2178457436992 -> 2178457437088
		2178457426816 [label="layer2.1.bn1.bias
 (32)" fillcolor=lightblue]
			2178457426816 -> 2178457436992
		2178457436992 [label=AccumulateGrad]
			2178457436896 -> 2178457436752
		2178457425408 [label="layer2.1.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			2178457425408 -> 2178457436896
		2178457436896 [label=AccumulateGrad]
			2178457436704 -> 2178457436608
		2178457426688 [label="layer2.1.bn2.weight
 (32)" fillcolor=lightblue]
			2178457426688 -> 2178457436704
		2178457436704 [label=AccumulateGrad]
			2178457436656 -> 2178457436608
		2178457424256 [label="layer2.1.bn2.bias
 (32)" fillcolor=lightblue]
			2178457424256 -> 2178457436656
		2178457436656 [label=AccumulateGrad]
			2178457436560 -> 2178457436512
			2178457436416 -> 2178457314736
		2178457425024 [label="layer2.2.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			2178457425024 -> 2178457436416
		2178457436416 [label=AccumulateGrad]
			2178457317280 -> 2178457317328
		2178457394624 [label="layer2.2.bn1.weight
 (32)" fillcolor=lightblue]
			2178457394624 -> 2178457317280
		2178457317280 [label=AccumulateGrad]
			2178457436224 -> 2178457317328
		2178457393600 [label="layer2.2.bn1.bias
 (32)" fillcolor=lightblue]
			2178457393600 -> 2178457436224
		2178457436224 [label=AccumulateGrad]
			2178457314688 -> 2178457314496
		2178457391808 [label="layer2.2.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			2178457391808 -> 2178457314688
		2178457314688 [label=AccumulateGrad]
			2178457314448 -> 2178457314304
		2178457391552 [label="layer2.2.bn2.weight
 (32)" fillcolor=lightblue]
			2178457391552 -> 2178457314448
		2178457314448 [label=AccumulateGrad]
			2178457314592 -> 2178457314304
		2178457394688 [label="layer2.2.bn2.bias
 (32)" fillcolor=lightblue]
			2178457394688 -> 2178457314592
		2178457314592 [label=AccumulateGrad]
			2178457314400 -> 2178603872208
			2178603872112 -> 2178603871968
		2178457391168 [label="layer3.0.conv1.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
			2178457391168 -> 2178603872112
		2178603872112 [label=AccumulateGrad]
			2178603871920 -> 2178603871872
		2178457391232 [label="layer3.0.bn1.weight
 (64)" fillcolor=lightblue]
			2178457391232 -> 2178603871920
		2178603871920 [label=AccumulateGrad]
			2178603871776 -> 2178603871872
		2178457392192 [label="layer3.0.bn1.bias
 (64)" fillcolor=lightblue]
			2178457392192 -> 2178603871776
		2178603871776 [label=AccumulateGrad]
			2178603871680 -> 2178603871536
		2178457337920 [label="layer3.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			2178457337920 -> 2178603871680
		2178603871680 [label=AccumulateGrad]
			2178603871488 -> 2178603871392
		2178457392448 [label="layer3.0.bn2.weight
 (64)" fillcolor=lightblue]
			2178457392448 -> 2178603871488
		2178603871488 [label=AccumulateGrad]
			2178603871440 -> 2178603871392
		2178457338176 [label="layer3.0.bn2.bias
 (64)" fillcolor=lightblue]
			2178457338176 -> 2178603871440
		2178603871440 [label=AccumulateGrad]
			2178603871344 -> 2178603871296
		2178603871344 [label=CudnnBatchNormBackward]
			2178603872064 -> 2178603871344
		2178603872064 [label=CudnnConvolutionBackward]
			2178603872160 -> 2178603872064
			2178603871824 -> 2178603872064
		2178457393152 [label="layer3.0.downsample.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
			2178457393152 -> 2178603871824
		2178603871824 [label=AccumulateGrad]
			2178603871632 -> 2178603871344
		2178457393280 [label="layer3.0.downsample.1.weight
 (64)" fillcolor=lightblue]
			2178457393280 -> 2178603871632
		2178603871632 [label=AccumulateGrad]
			2178603871584 -> 2178603871344
		2178457392768 [label="layer3.0.downsample.1.bias
 (64)" fillcolor=lightblue]
			2178457392768 -> 2178603871584
		2178603871584 [label=AccumulateGrad]
			2178603871200 -> 2178603871008
		2178457338752 [label="layer3.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			2178457338752 -> 2178603871200
		2178603871200 [label=AccumulateGrad]
			2178603870960 -> 2178603870912
		2178457338688 [label="layer3.1.bn1.weight
 (64)" fillcolor=lightblue]
			2178457338688 -> 2178603870960
		2178603870960 [label=AccumulateGrad]
			2178603870816 -> 2178603870912
		2178457339008 [label="layer3.1.bn1.bias
 (64)" fillcolor=lightblue]
			2178457339008 -> 2178603870816
		2178603870816 [label=AccumulateGrad]
			2178603870720 -> 2178603870576
		2178457339712 [label="layer3.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			2178457339712 -> 2178603870720
		2178603870720 [label=AccumulateGrad]
			2178603870528 -> 2178603870432
		2178457339072 [label="layer3.1.bn2.weight
 (64)" fillcolor=lightblue]
			2178457339072 -> 2178603870528
		2178603870528 [label=AccumulateGrad]
			2178603870480 -> 2178603870432
		2178457340032 [label="layer3.1.bn2.bias
 (64)" fillcolor=lightblue]
			2178457340032 -> 2178603870480
		2178603870480 [label=AccumulateGrad]
			2178603870384 -> 2178603870336
			2178603870240 -> 2178603870048
		2178457340608 [label="layer3.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			2178457340608 -> 2178603870240
		2178603870240 [label=AccumulateGrad]
			2178603870000 -> 2178603869952
		2178457340544 [label="layer3.2.bn1.weight
 (64)" fillcolor=lightblue]
			2178457340544 -> 2178603870000
		2178603870000 [label=AccumulateGrad]
			2178603869856 -> 2178603869952
		2178457340864 [label="layer3.2.bn1.bias
 (64)" fillcolor=lightblue]
			2178457340864 -> 2178603869856
		2178603869856 [label=AccumulateGrad]
			2178603869760 -> 2178603869616
		2178457341568 [label="layer3.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			2178457341568 -> 2178603869760
		2178603869760 [label=AccumulateGrad]
			2178603869568 -> 2178603869472
		2178457340928 [label="layer3.2.bn2.weight
 (64)" fillcolor=lightblue]
			2178457340928 -> 2178603869568
		2178603869568 [label=AccumulateGrad]
			2178603869520 -> 2178603869472
		2178457341888 [label="layer3.2.bn2.bias
 (64)" fillcolor=lightblue]
			2178457341888 -> 2178603869520
		2178603869520 [label=AccumulateGrad]
			2178603869424 -> 2178603869376
			2178603868800 -> 2178603868944
		2178603868800 [label=TBackward]
			2178603869328 -> 2178603868800
		2178457468288 [label="fc.weight
 (100, 64)" fillcolor=lightblue]
			2178457468288 -> 2178603869328
		2178603869328 [label=AccumulateGrad]
			2178603868944 -> 2178457392896
}