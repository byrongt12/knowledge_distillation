digraph {
	graph [size="325.95,325.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
		1781570756224 [label="
 (100, 100)" fillcolor=darkolivegreen1]
		1785186375904 [label=AddmmBackward]
			1785186376960 -> 1785186375904
		1781568564864 [label="fc.bias
 (100)" fillcolor=lightblue]
			1781568564864 -> 1785186376960
		1785186376960 [label=AccumulateGrad]
			1785186376096 -> 1785186375904
		1785186376096 [label=ViewBackward]
			1785186375136 -> 1785186376096
		1785186375136 [label=AvgPool2DBackward]
			1785186375328 -> 1785186375136
		1785186375328 [label=ReluBackward1]
			1785186374080 -> 1785186375328
		1785186374080 [label=AddBackward0]
			1785186376576 -> 1785186374080
		1785186376576 [label=CudnnBatchNormBackward]
			1785186373984 -> 1785186376576
		1785186373984 [label=CudnnConvolutionBackward]
			1785186374560 -> 1785186373984
		1785186374560 [label=ReluBackward1]
			1785186374464 -> 1785186374560
		1785186374464 [label=CudnnBatchNormBackward]
			1785186376480 -> 1785186374464
		1785186376480 [label=CudnnConvolutionBackward]
			1785186376288 -> 1785186376480
		1785186376288 [label=ReluBackward1]
			1785186374752 -> 1785186376288
		1785186374752 [label=AddBackward0]
			1785186373888 -> 1785186374752
		1785186373888 [label=CudnnBatchNormBackward]
			1782065467248 -> 1785186373888
		1782065467248 [label=CudnnConvolutionBackward]
			1782065467056 -> 1782065467248
		1782065467056 [label=ReluBackward1]
			1782065466768 -> 1782065467056
		1782065466768 [label=CudnnBatchNormBackward]
			1782065466576 -> 1782065466768
		1782065466576 [label=CudnnConvolutionBackward]
			1785186376672 -> 1782065466576
		1785186376672 [label=ReluBackward1]
			1782065466000 -> 1785186376672
		1782065466000 [label=AddBackward0]
			1782065465808 -> 1782065466000
		1782065465808 [label=CudnnBatchNormBackward]
			1782065465520 -> 1782065465808
		1782065465520 [label=CudnnConvolutionBackward]
			1782065465136 -> 1782065465520
		1782065465136 [label=ReluBackward1]
			1782065464848 -> 1782065465136
		1782065464848 [label=CudnnBatchNormBackward]
			1782065464656 -> 1782065464848
		1782065464656 [label=CudnnConvolutionBackward]
			1782065465904 -> 1782065464656
		1782065465904 [label=ReluBackward1]
			1782065464080 -> 1782065465904
		1782065464080 [label=AddBackward0]
			1782065463888 -> 1782065464080
		1782065463888 [label=CudnnBatchNormBackward]
			1782065467104 -> 1782065463888
		1782065467104 [label=CudnnConvolutionBackward]
			1785186024608 -> 1782065467104
		1785186024608 [label=ReluBackward1]
			1785186024560 -> 1785186024608
		1785186024560 [label=CudnnBatchNormBackward]
			1785186024992 -> 1785186024560
		1785186024992 [label=CudnnConvolutionBackward]
			1782065463984 -> 1785186024992
		1782065463984 [label=ReluBackward1]
			1785186307664 -> 1782065463984
		1785186307664 [label=AddBackward0]
			1785186307808 -> 1785186307664
		1785186307808 [label=CudnnBatchNormBackward]
			1785186307952 -> 1785186307808
		1785186307952 [label=CudnnConvolutionBackward]
			1785186304352 -> 1785186307952
		1785186304352 [label=ReluBackward1]
			1785186304208 -> 1785186304352
		1785186304208 [label=CudnnBatchNormBackward]
			1785186304544 -> 1785186304208
		1785186304544 [label=CudnnConvolutionBackward]
			1785186307232 -> 1785186304544
		1785186307232 [label=ReluBackward1]
			1785186424144 -> 1785186307232
		1785186424144 [label=AddBackward0]
			1785186424000 -> 1785186424144
		1785186424000 [label=CudnnBatchNormBackward]
			1785186423712 -> 1785186424000
		1785186423712 [label=CudnnConvolutionBackward]
			1785186423328 -> 1785186423712
		1785186423328 [label=ReluBackward1]
			1785186423040 -> 1785186423328
		1785186423040 [label=CudnnBatchNormBackward]
			1785186424384 -> 1785186423040
		1785186424384 [label=CudnnConvolutionBackward]
			1785186425824 -> 1785186424384
		1785186425824 [label=ReluBackward1]
			1785186425968 -> 1785186425824
		1785186425968 [label=AddBackward0]
			1782065682320 -> 1785186425968
		1782065682320 [label=CudnnBatchNormBackward]
			1782065682464 -> 1782065682320
		1782065682464 [label=CudnnConvolutionBackward]
			1782065682656 -> 1782065682464
		1782065682656 [label=ReluBackward1]
			1782065682800 -> 1782065682656
		1782065682800 [label=CudnnBatchNormBackward]
			1782065682896 -> 1782065682800
		1782065682896 [label=CudnnConvolutionBackward]
			1782065682272 -> 1782065682896
		1782065682272 [label=ReluBackward1]
			1782065683184 -> 1782065682272
		1782065683184 [label=AddBackward0]
			1782065683280 -> 1782065683184
		1782065683280 [label=CudnnBatchNormBackward]
			1782065683424 -> 1782065683280
		1782065683424 [label=CudnnConvolutionBackward]
			1782065683616 -> 1782065683424
		1782065683616 [label=ReluBackward1]
			1782065683760 -> 1782065683616
		1782065683760 [label=CudnnBatchNormBackward]
			1782065683856 -> 1782065683760
		1782065683856 [label=CudnnConvolutionBackward]
			1782065683232 -> 1782065683856
		1782065683232 [label=ReluBackward1]
			1782065684144 -> 1782065683232
		1782065684144 [label=AddBackward0]
			1782065684240 -> 1782065684144
		1782065684240 [label=CudnnBatchNormBackward]
			1782065684384 -> 1782065684240
		1782065684384 [label=CudnnConvolutionBackward]
			1785186402464 -> 1782065684384
		1785186402464 [label=ReluBackward1]
			1785186402608 -> 1785186402464
		1785186402608 [label=CudnnBatchNormBackward]
			1785186402704 -> 1785186402608
		1785186402704 [label=CudnnConvolutionBackward]
			1782065684192 -> 1785186402704
		1782065684192 [label=ReluBackward1]
			1785186402992 -> 1782065684192
		1785186402992 [label=AddBackward0]
			1785186403088 -> 1785186402992
		1785186403088 [label=CudnnBatchNormBackward]
			1785186403232 -> 1785186403088
		1785186403232 [label=CudnnConvolutionBackward]
			1785186403424 -> 1785186403232
		1785186403424 [label=ReluBackward1]
			1785186403568 -> 1785186403424
		1785186403568 [label=CudnnBatchNormBackward]
			1785186403664 -> 1785186403568
		1785186403664 [label=CudnnConvolutionBackward]
			1785186403040 -> 1785186403664
		1785186403040 [label=ReluBackward1]
			1785186403952 -> 1785186403040
		1785186403952 [label=AddBackward0]
			1785186404048 -> 1785186403952
		1785186404048 [label=CudnnBatchNormBackward]
			1785186404192 -> 1785186404048
		1785186404192 [label=CudnnConvolutionBackward]
			1785186404384 -> 1785186404192
		1785186404384 [label=ReluBackward1]
			1785186404528 -> 1785186404384
		1785186404528 [label=CudnnBatchNormBackward]
			1785186404624 -> 1785186404528
		1785186404624 [label=CudnnConvolutionBackward]
			1785186404000 -> 1785186404624
		1785186404000 [label=ReluBackward1]
			1785186404912 -> 1785186404000
		1785186404912 [label=AddBackward0]
			1785186405008 -> 1785186404912
		1785186405008 [label=CudnnBatchNormBackward]
			1785186405152 -> 1785186405008
		1785186405152 [label=CudnnConvolutionBackward]
			1785186405344 -> 1785186405152
		1785186405344 [label=ReluBackward1]
			1785186405488 -> 1785186405344
		1785186405488 [label=CudnnBatchNormBackward]
			1785186405584 -> 1785186405488
		1785186405584 [label=CudnnConvolutionBackward]
			1785186404960 -> 1785186405584
		1785186404960 [label=ReluBackward1]
			1785186405872 -> 1785186404960
		1785186405872 [label=AddBackward0]
			1785186405968 -> 1785186405872
		1785186405968 [label=CudnnBatchNormBackward]
			1785186406112 -> 1785186405968
		1785186406112 [label=CudnnConvolutionBackward]
			1785186406304 -> 1785186406112
		1785186406304 [label=ReluBackward1]
			1785186406352 -> 1785186406304
		1785186406352 [label=CudnnBatchNormBackward]
			1785186357456 -> 1785186406352
		1785186357456 [label=CudnnConvolutionBackward]
			1785186405920 -> 1785186357456
		1785186405920 [label=ReluBackward1]
			1785186357744 -> 1785186405920
		1785186357744 [label=AddBackward0]
			1785186357840 -> 1785186357744
		1785186357840 [label=CudnnBatchNormBackward]
			1785186357984 -> 1785186357840
		1785186357984 [label=CudnnConvolutionBackward]
			1785186358176 -> 1785186357984
		1785186358176 [label=ReluBackward1]
			1785186358320 -> 1785186358176
		1785186358320 [label=CudnnBatchNormBackward]
			1785186358416 -> 1785186358320
		1785186358416 [label=CudnnConvolutionBackward]
			1785186357792 -> 1785186358416
		1785186357792 [label=ReluBackward1]
			1785186358704 -> 1785186357792
		1785186358704 [label=AddBackward0]
			1785186358800 -> 1785186358704
		1785186358800 [label=CudnnBatchNormBackward]
			1785186358944 -> 1785186358800
		1785186358944 [label=CudnnConvolutionBackward]
			1785186359136 -> 1785186358944
		1785186359136 [label=ReluBackward1]
			1785186359280 -> 1785186359136
		1785186359280 [label=CudnnBatchNormBackward]
			1785186359376 -> 1785186359280
		1785186359376 [label=CudnnConvolutionBackward]
			1785186358752 -> 1785186359376
		1785186358752 [label=ReluBackward1]
			1785186359664 -> 1785186358752
		1785186359664 [label=AddBackward0]
			1785186359760 -> 1785186359664
		1785186359760 [label=CudnnBatchNormBackward]
			1785186359904 -> 1785186359760
		1785186359904 [label=CudnnConvolutionBackward]
			1785186360096 -> 1785186359904
		1785186360096 [label=ReluBackward1]
			1785186360240 -> 1785186360096
		1785186360240 [label=CudnnBatchNormBackward]
			1785186360336 -> 1785186360240
		1785186360336 [label=CudnnConvolutionBackward]
			1785186359712 -> 1785186360336
		1785186359712 [label=ReluBackward1]
			1785186360624 -> 1785186359712
		1785186360624 [label=AddBackward0]
			1785186360720 -> 1785186360624
		1785186360720 [label=CudnnBatchNormBackward]
			1785186360864 -> 1785186360720
		1785186360864 [label=CudnnConvolutionBackward]
			1785186361056 -> 1785186360864
		1785186361056 [label=ReluBackward1]
			1785186361200 -> 1785186361056
		1785186361200 [label=CudnnBatchNormBackward]
			1785186361296 -> 1785186361200
		1785186361296 [label=CudnnConvolutionBackward]
			1785186360672 -> 1785186361296
		1785186360672 [label=ReluBackward1]
			1785186369840 -> 1785186360672
		1785186369840 [label=AddBackward0]
			1785186369936 -> 1785186369840
		1785186369936 [label=CudnnBatchNormBackward]
			1785186370080 -> 1785186369936
		1785186370080 [label=CudnnConvolutionBackward]
			1785186370272 -> 1785186370080
		1785186370272 [label=ReluBackward1]
			1785186370416 -> 1785186370272
		1785186370416 [label=CudnnBatchNormBackward]
			1785186370512 -> 1785186370416
		1785186370512 [label=CudnnConvolutionBackward]
			1785186370704 -> 1785186370512
		1785186370704 [label=ReluBackward1]
			1785186370848 -> 1785186370704
		1785186370848 [label=AddBackward0]
			1785186370944 -> 1785186370848
		1785186370944 [label=CudnnBatchNormBackward]
			1785186371088 -> 1785186370944
		1785186371088 [label=CudnnConvolutionBackward]
			1785186371280 -> 1785186371088
		1785186371280 [label=ReluBackward1]
			1785186371424 -> 1785186371280
		1785186371424 [label=CudnnBatchNormBackward]
			1785186371520 -> 1785186371424
		1785186371520 [label=CudnnConvolutionBackward]
			1785186370896 -> 1785186371520
		1785186370896 [label=ReluBackward1]
			1785186371808 -> 1785186370896
		1785186371808 [label=AddBackward0]
			1785186371904 -> 1785186371808
		1785186371904 [label=CudnnBatchNormBackward]
			1785186372048 -> 1785186371904
		1785186372048 [label=CudnnConvolutionBackward]
			1785186372240 -> 1785186372048
		1785186372240 [label=ReluBackward1]
			1785186372384 -> 1785186372240
		1785186372384 [label=CudnnBatchNormBackward]
			1785186372480 -> 1785186372384
		1785186372480 [label=CudnnConvolutionBackward]
			1785186371856 -> 1785186372480
		1785186371856 [label=ReluBackward1]
			1785186372768 -> 1785186371856
		1785186372768 [label=AddBackward0]
			1785186372864 -> 1785186372768
		1785186372864 [label=CudnnBatchNormBackward]
			1785186373008 -> 1785186372864
		1785186373008 [label=CudnnConvolutionBackward]
			1785186373200 -> 1785186373008
		1785186373200 [label=ReluBackward1]
			1785186373344 -> 1785186373200
		1785186373344 [label=CudnnBatchNormBackward]
			1785186373440 -> 1785186373344
		1785186373440 [label=CudnnConvolutionBackward]
			1785186372816 -> 1785186373440
		1785186372816 [label=ReluBackward1]
			1785186341024 -> 1785186372816
		1785186341024 [label=AddBackward0]
			1785186341120 -> 1785186341024
		1785186341120 [label=CudnnBatchNormBackward]
			1785186341264 -> 1785186341120
		1785186341264 [label=CudnnConvolutionBackward]
			1785186341456 -> 1785186341264
		1785186341456 [label=ReluBackward1]
			1785186341600 -> 1785186341456
		1785186341600 [label=CudnnBatchNormBackward]
			1785186341696 -> 1785186341600
		1785186341696 [label=CudnnConvolutionBackward]
			1785186341072 -> 1785186341696
		1785186341072 [label=ReluBackward1]
			1785186341984 -> 1785186341072
		1785186341984 [label=AddBackward0]
			1785186342080 -> 1785186341984
		1785186342080 [label=CudnnBatchNormBackward]
			1785186342224 -> 1785186342080
		1785186342224 [label=CudnnConvolutionBackward]
			1785186342416 -> 1785186342224
		1785186342416 [label=ReluBackward1]
			1785186342560 -> 1785186342416
		1785186342560 [label=CudnnBatchNormBackward]
			1785186342656 -> 1785186342560
		1785186342656 [label=CudnnConvolutionBackward]
			1785186342032 -> 1785186342656
		1785186342032 [label=ReluBackward1]
			1785186342944 -> 1785186342032
		1785186342944 [label=AddBackward0]
			1785186343040 -> 1785186342944
		1785186343040 [label=CudnnBatchNormBackward]
			1785186343184 -> 1785186343040
		1785186343184 [label=CudnnConvolutionBackward]
			1785186343376 -> 1785186343184
		1785186343376 [label=ReluBackward1]
			1785186343520 -> 1785186343376
		1785186343520 [label=CudnnBatchNormBackward]
			1785186343616 -> 1785186343520
		1785186343616 [label=CudnnConvolutionBackward]
			1785186342992 -> 1785186343616
		1785186342992 [label=ReluBackward1]
			1785186343904 -> 1785186342992
		1785186343904 [label=AddBackward0]
			1785186344000 -> 1785186343904
		1785186344000 [label=CudnnBatchNormBackward]
			1785186344144 -> 1785186344000
		1785186344144 [label=CudnnConvolutionBackward]
			1785186344336 -> 1785186344144
		1785186344336 [label=ReluBackward1]
			1785186344480 -> 1785186344336
		1785186344480 [label=CudnnBatchNormBackward]
			1785186344576 -> 1785186344480
		1785186344576 [label=CudnnConvolutionBackward]
			1785186343952 -> 1785186344576
		1785186343952 [label=ReluBackward1]
			1785186344864 -> 1785186343952
		1785186344864 [label=AddBackward0]
			1785186344912 -> 1785186344864
		1785186344912 [label=CudnnBatchNormBackward]
			1781571473616 -> 1785186344912
		1781571473616 [label=CudnnConvolutionBackward]
			1781571473808 -> 1781571473616
		1781571473808 [label=ReluBackward1]
			1781571473952 -> 1781571473808
		1781571473952 [label=CudnnBatchNormBackward]
			1781571474048 -> 1781571473952
		1781571474048 [label=CudnnConvolutionBackward]
			1785186344672 -> 1781571474048
		1785186344672 [label=ReluBackward1]
			1781571474336 -> 1785186344672
		1781571474336 [label=AddBackward0]
			1781571474432 -> 1781571474336
		1781571474432 [label=CudnnBatchNormBackward]
			1781571474576 -> 1781571474432
		1781571474576 [label=CudnnConvolutionBackward]
			1781571474768 -> 1781571474576
		1781571474768 [label=ReluBackward1]
			1781571474912 -> 1781571474768
		1781571474912 [label=CudnnBatchNormBackward]
			1781571475008 -> 1781571474912
		1781571475008 [label=CudnnConvolutionBackward]
			1781571474384 -> 1781571475008
		1781571474384 [label=ReluBackward1]
			1781571475296 -> 1781571474384
		1781571475296 [label=AddBackward0]
			1781571475392 -> 1781571475296
		1781571475392 [label=CudnnBatchNormBackward]
			1781571475536 -> 1781571475392
		1781571475536 [label=CudnnConvolutionBackward]
			1781571475728 -> 1781571475536
		1781571475728 [label=ReluBackward1]
			1781571475872 -> 1781571475728
		1781571475872 [label=CudnnBatchNormBackward]
			1781571475968 -> 1781571475872
		1781571475968 [label=CudnnConvolutionBackward]
			1781571475344 -> 1781571475968
		1781571475344 [label=ReluBackward1]
			1781571476256 -> 1781571475344
		1781571476256 [label=AddBackward0]
			1781571476352 -> 1781571476256
		1781571476352 [label=CudnnBatchNormBackward]
			1781571476496 -> 1781571476352
		1781571476496 [label=CudnnConvolutionBackward]
			1781571476688 -> 1781571476496
		1781571476688 [label=ReluBackward1]
			1781571476832 -> 1781571476688
		1781571476832 [label=CudnnBatchNormBackward]
			1781571476928 -> 1781571476832
		1781571476928 [label=CudnnConvolutionBackward]
			1781571476304 -> 1781571476928
		1781571476304 [label=ReluBackward1]
			1781571477216 -> 1781571476304
		1781571477216 [label=AddBackward0]
			1781571477312 -> 1781571477216
		1781571477312 [label=CudnnBatchNormBackward]
			1781571477456 -> 1781571477312
		1781571477456 [label=CudnnConvolutionBackward]
			1781571481808 -> 1781571477456
		1781571481808 [label=ReluBackward1]
			1781571481952 -> 1781571481808
		1781571481952 [label=CudnnBatchNormBackward]
			1781571482048 -> 1781571481952
		1781571482048 [label=CudnnConvolutionBackward]
			1781571477264 -> 1781571482048
		1781571477264 [label=ReluBackward1]
			1781571482336 -> 1781571477264
		1781571482336 [label=AddBackward0]
			1781571482432 -> 1781571482336
		1781571482432 [label=CudnnBatchNormBackward]
			1781571482576 -> 1781571482432
		1781571482576 [label=CudnnConvolutionBackward]
			1781571482768 -> 1781571482576
		1781571482768 [label=ReluBackward1]
			1781571482912 -> 1781571482768
		1781571482912 [label=CudnnBatchNormBackward]
			1781571483008 -> 1781571482912
		1781571483008 [label=CudnnConvolutionBackward]
			1781571482384 -> 1781571483008
		1781571482384 [label=ReluBackward1]
			1781571483296 -> 1781571482384
		1781571483296 [label=AddBackward0]
			1781571483392 -> 1781571483296
		1781571483392 [label=CudnnBatchNormBackward]
			1781571483536 -> 1781571483392
		1781571483536 [label=CudnnConvolutionBackward]
			1781571483728 -> 1781571483536
		1781571483728 [label=ReluBackward1]
			1781571483872 -> 1781571483728
		1781571483872 [label=CudnnBatchNormBackward]
			1781571483968 -> 1781571483872
		1781571483968 [label=CudnnConvolutionBackward]
			1781571483344 -> 1781571483968
		1781571483344 [label=ReluBackward1]
			1781571484256 -> 1781571483344
		1781571484256 [label=AddBackward0]
			1781571484352 -> 1781571484256
		1781571484352 [label=CudnnBatchNormBackward]
			1781571484496 -> 1781571484352
		1781571484496 [label=CudnnConvolutionBackward]
			1781571484688 -> 1781571484496
		1781571484688 [label=ReluBackward1]
			1781571484832 -> 1781571484688
		1781571484832 [label=CudnnBatchNormBackward]
			1781571484928 -> 1781571484832
		1781571484928 [label=CudnnConvolutionBackward]
			1781571484304 -> 1781571484928
		1781571484304 [label=ReluBackward1]
			1781571485216 -> 1781571484304
		1781571485216 [label=AddBackward0]
			1781571485312 -> 1781571485216
		1781571485312 [label=CudnnBatchNormBackward]
			1781571485456 -> 1781571485312
		1781571485456 [label=CudnnConvolutionBackward]
			1781571485648 -> 1781571485456
		1781571485648 [label=ReluBackward1]
			1781571440800 -> 1781571485648
		1781571440800 [label=CudnnBatchNormBackward]
			1781571440896 -> 1781571440800
		1781571440896 [label=CudnnConvolutionBackward]
			1781571485264 -> 1781571440896
		1781571485264 [label=ReluBackward1]
			1781571441184 -> 1781571485264
		1781571441184 [label=AddBackward0]
			1781571441280 -> 1781571441184
		1781571441280 [label=CudnnBatchNormBackward]
			1781571441424 -> 1781571441280
		1781571441424 [label=CudnnConvolutionBackward]
			1781571441616 -> 1781571441424
		1781571441616 [label=ReluBackward1]
			1781571441760 -> 1781571441616
		1781571441760 [label=CudnnBatchNormBackward]
			1781571441856 -> 1781571441760
		1781571441856 [label=CudnnConvolutionBackward]
			1781571441232 -> 1781571441856
		1781571441232 [label=ReluBackward1]
			1781571442144 -> 1781571441232
		1781571442144 [label=AddBackward0]
			1781571442240 -> 1781571442144
		1781571442240 [label=CudnnBatchNormBackward]
			1781571442384 -> 1781571442240
		1781571442384 [label=CudnnConvolutionBackward]
			1781571442576 -> 1781571442384
		1781571442576 [label=ReluBackward1]
			1781571442720 -> 1781571442576
		1781571442720 [label=CudnnBatchNormBackward]
			1781571442816 -> 1781571442720
		1781571442816 [label=CudnnConvolutionBackward]
			1781571443008 -> 1781571442816
		1781571443008 [label=ReluBackward1]
			1781571443152 -> 1781571443008
		1781571443152 [label=AddBackward0]
			1781571443248 -> 1781571443152
		1781571443248 [label=CudnnBatchNormBackward]
			1781571443392 -> 1781571443248
		1781571443392 [label=CudnnConvolutionBackward]
			1781571443584 -> 1781571443392
		1781571443584 [label=ReluBackward1]
			1781571443728 -> 1781571443584
		1781571443728 [label=CudnnBatchNormBackward]
			1781571443824 -> 1781571443728
		1781571443824 [label=CudnnConvolutionBackward]
			1781571443200 -> 1781571443824
		1781571443200 [label=ReluBackward1]
			1781571444112 -> 1781571443200
		1781571444112 [label=AddBackward0]
			1781571444208 -> 1781571444112
		1781571444208 [label=CudnnBatchNormBackward]
			1781571444352 -> 1781571444208
		1781571444352 [label=CudnnConvolutionBackward]
			1781571444544 -> 1781571444352
		1781571444544 [label=ReluBackward1]
			1781571444688 -> 1781571444544
		1781571444688 [label=CudnnBatchNormBackward]
			1781571444592 -> 1781571444688
		1781571444592 [label=CudnnConvolutionBackward]
			1781571444160 -> 1781571444592
		1781571444160 [label=ReluBackward1]
			1781571416464 -> 1781571444160
		1781571416464 [label=AddBackward0]
			1781571416560 -> 1781571416464
		1781571416560 [label=CudnnBatchNormBackward]
			1781571416704 -> 1781571416560
		1781571416704 [label=CudnnConvolutionBackward]
			1781571416896 -> 1781571416704
		1781571416896 [label=ReluBackward1]
			1781571417040 -> 1781571416896
		1781571417040 [label=CudnnBatchNormBackward]
			1781571417136 -> 1781571417040
		1781571417136 [label=CudnnConvolutionBackward]
			1781571416512 -> 1781571417136
		1781571416512 [label=ReluBackward1]
			1781571417424 -> 1781571416512
		1781571417424 [label=AddBackward0]
			1781571417520 -> 1781571417424
		1781571417520 [label=CudnnBatchNormBackward]
			1781571417664 -> 1781571417520
		1781571417664 [label=CudnnConvolutionBackward]
			1781571417856 -> 1781571417664
		1781571417856 [label=ReluBackward1]
			1781571418000 -> 1781571417856
		1781571418000 [label=CudnnBatchNormBackward]
			1781571418096 -> 1781571418000
		1781571418096 [label=CudnnConvolutionBackward]
			1781571417472 -> 1781571418096
		1781571417472 [label=ReluBackward1]
			1781571418384 -> 1781571417472
		1781571418384 [label=AddBackward0]
			1781571418480 -> 1781571418384
		1781571418480 [label=CudnnBatchNormBackward]
			1781571418624 -> 1781571418480
		1781571418624 [label=CudnnConvolutionBackward]
			1781571418816 -> 1781571418624
		1781571418816 [label=ReluBackward1]
			1781571418960 -> 1781571418816
		1781571418960 [label=CudnnBatchNormBackward]
			1781571419056 -> 1781571418960
		1781571419056 [label=CudnnConvolutionBackward]
			1781571418432 -> 1781571419056
		1781571418432 [label=ReluBackward1]
			1781571419344 -> 1781571418432
		1781571419344 [label=AddBackward0]
			1781571419440 -> 1781571419344
		1781571419440 [label=CudnnBatchNormBackward]
			1781571419584 -> 1781571419440
		1781571419584 [label=CudnnConvolutionBackward]
			1781571419776 -> 1781571419584
		1781571419776 [label=ReluBackward1]
			1781571419920 -> 1781571419776
		1781571419920 [label=CudnnBatchNormBackward]
			1781571420016 -> 1781571419920
		1781571420016 [label=CudnnConvolutionBackward]
			1781571419392 -> 1781571420016
		1781571419392 [label=ReluBackward1]
			1781571403984 -> 1781571419392
		1781571403984 [label=AddBackward0]
			1781571404080 -> 1781571403984
		1781571404080 [label=CudnnBatchNormBackward]
			1781571404224 -> 1781571404080
		1781571404224 [label=CudnnConvolutionBackward]
			1781571404416 -> 1781571404224
		1781571404416 [label=ReluBackward1]
			1781571404560 -> 1781571404416
		1781571404560 [label=CudnnBatchNormBackward]
			1781571404656 -> 1781571404560
		1781571404656 [label=CudnnConvolutionBackward]
			1781571404032 -> 1781571404656
		1781571404032 [label=ReluBackward1]
			1781571404944 -> 1781571404032
		1781571404944 [label=AddBackward0]
			1781571405040 -> 1781571404944
		1781571405040 [label=CudnnBatchNormBackward]
			1781571405184 -> 1781571405040
		1781571405184 [label=CudnnConvolutionBackward]
			1781571405376 -> 1781571405184
		1781571405376 [label=ReluBackward1]
			1781571405520 -> 1781571405376
		1781571405520 [label=CudnnBatchNormBackward]
			1781571405616 -> 1781571405520
		1781571405616 [label=CudnnConvolutionBackward]
			1781571404992 -> 1781571405616
		1781571404992 [label=ReluBackward1]
			1781571405904 -> 1781571404992
		1781571405904 [label=AddBackward0]
			1781571406000 -> 1781571405904
		1781571406000 [label=CudnnBatchNormBackward]
			1781571406144 -> 1781571406000
		1781571406144 [label=CudnnConvolutionBackward]
			1781571406336 -> 1781571406144
		1781571406336 [label=ReluBackward1]
			1781571406480 -> 1781571406336
		1781571406480 [label=CudnnBatchNormBackward]
			1781571406576 -> 1781571406480
		1781571406576 [label=CudnnConvolutionBackward]
			1781571405952 -> 1781571406576
		1781571405952 [label=ReluBackward1]
			1781571406864 -> 1781571405952
		1781571406864 [label=AddBackward0]
			1781571406960 -> 1781571406864
		1781571406960 [label=CudnnBatchNormBackward]
			1781571407104 -> 1781571406960
		1781571407104 [label=CudnnConvolutionBackward]
			1781571407296 -> 1781571407104
		1781571407296 [label=ReluBackward1]
			1781571407440 -> 1781571407296
		1781571407440 [label=CudnnBatchNormBackward]
			1781571407536 -> 1781571407440
		1781571407536 [label=CudnnConvolutionBackward]
			1781571406912 -> 1781571407536
		1781571406912 [label=ReluBackward1]
			1781571407824 -> 1781571406912
		1781571407824 [label=AddBackward0]
			1781571407632 -> 1781571407824
		1781571407632 [label=CudnnBatchNormBackward]
			1781571379456 -> 1781571407632
		1781571379456 [label=CudnnConvolutionBackward]
			1781571379648 -> 1781571379456
		1781571379648 [label=ReluBackward1]
			1781571379792 -> 1781571379648
		1781571379792 [label=CudnnBatchNormBackward]
			1781571379888 -> 1781571379792
		1781571379888 [label=CudnnConvolutionBackward]
			1781571379312 -> 1781571379888
		1781571379312 [label=ReluBackward1]
			1781571380176 -> 1781571379312
		1781571380176 [label=AddBackward0]
			1781571380272 -> 1781571380176
		1781571380272 [label=CudnnBatchNormBackward]
			1781571380416 -> 1781571380272
		1781571380416 [label=CudnnConvolutionBackward]
			1781571380608 -> 1781571380416
		1781571380608 [label=ReluBackward1]
			1781571380752 -> 1781571380608
		1781571380752 [label=CudnnBatchNormBackward]
			1781571380848 -> 1781571380752
		1781571380848 [label=CudnnConvolutionBackward]
			1781571380224 -> 1781571380848
		1781571380224 [label=ReluBackward1]
			1781571381136 -> 1781571380224
		1781571381136 [label=AddBackward0]
			1781571381232 -> 1781571381136
		1781571381232 [label=CudnnBatchNormBackward]
			1781571381376 -> 1781571381232
		1781571381376 [label=CudnnConvolutionBackward]
			1781571381568 -> 1781571381376
		1781571381568 [label=ReluBackward1]
			1781571381712 -> 1781571381568
		1781571381712 [label=CudnnBatchNormBackward]
			1781571381808 -> 1781571381712
		1781571381808 [label=CudnnConvolutionBackward]
			1781571381184 -> 1781571381808
		1781571381184 [label=ReluBackward1]
			1781571382096 -> 1781571381184
		1781571382096 [label=AddBackward0]
			1781571382192 -> 1781571382096
		1781571382192 [label=CudnnBatchNormBackward]
			1781571382336 -> 1781571382192
		1781571382336 [label=CudnnConvolutionBackward]
			1781571382528 -> 1781571382336
		1781571382528 [label=ReluBackward1]
			1781571382672 -> 1781571382528
		1781571382672 [label=CudnnBatchNormBackward]
			1781571382768 -> 1781571382672
		1781571382768 [label=CudnnConvolutionBackward]
			1781571382144 -> 1781571382768
		1781571382144 [label=ReluBackward1]
			1781571383056 -> 1781571382144
		1781571383056 [label=AddBackward0]
			1781571383152 -> 1781571383056
		1781571383152 [label=CudnnBatchNormBackward]
			1781571383248 -> 1781571383152
		1781571383248 [label=CudnnConvolutionBackward]
			1780361830656 -> 1781571383248
		1780361830656 [label=ReluBackward1]
			1780361830800 -> 1780361830656
		1780361830800 [label=CudnnBatchNormBackward]
			1780361830896 -> 1780361830800
		1780361830896 [label=CudnnConvolutionBackward]
			1781571383104 -> 1780361830896
		1781571383104 [label=ReluBackward1]
			1780361831184 -> 1781571383104
		1780361831184 [label=AddBackward0]
			1780361831280 -> 1780361831184
		1780361831280 [label=CudnnBatchNormBackward]
			1780361831424 -> 1780361831280
		1780361831424 [label=CudnnConvolutionBackward]
			1780361831616 -> 1780361831424
		1780361831616 [label=ReluBackward1]
			1780361831760 -> 1780361831616
		1780361831760 [label=CudnnBatchNormBackward]
			1780361831856 -> 1780361831760
		1780361831856 [label=CudnnConvolutionBackward]
			1780361831232 -> 1780361831856
		1780361831232 [label=ReluBackward1]
			1780361832144 -> 1780361831232
		1780361832144 [label=AddBackward0]
			1780361832240 -> 1780361832144
		1780361832240 [label=CudnnBatchNormBackward]
			1780361832384 -> 1780361832240
		1780361832384 [label=CudnnConvolutionBackward]
			1780361832576 -> 1780361832384
		1780361832576 [label=ReluBackward1]
			1780361832720 -> 1780361832576
		1780361832720 [label=CudnnBatchNormBackward]
			1780361832816 -> 1780361832720
		1780361832816 [label=CudnnConvolutionBackward]
			1780361832192 -> 1780361832816
		1780361832192 [label=ReluBackward1]
			1780361833104 -> 1780361832192
		1780361833104 [label=AddBackward0]
			1780361833200 -> 1780361833104
		1780361833200 [label=CudnnBatchNormBackward]
			1780361833344 -> 1780361833200
		1780361833344 [label=CudnnConvolutionBackward]
			1780361833536 -> 1780361833344
		1780361833536 [label=ReluBackward1]
			1780361833680 -> 1780361833536
		1780361833680 [label=CudnnBatchNormBackward]
			1780361833776 -> 1780361833680
		1780361833776 [label=CudnnConvolutionBackward]
			1780361833152 -> 1780361833776
		1780361833152 [label=ReluBackward1]
			1780361834064 -> 1780361833152
		1780361834064 [label=CudnnBatchNormBackward]
			1780361834160 -> 1780361834064
		1780361834160 [label=CudnnConvolutionBackward]
			1780361834352 -> 1780361834160
		1781570399360 [label="conv.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
			1781570399360 -> 1780361834352
		1780361834352 [label=AccumulateGrad]
			1780361834112 -> 1780361834064
		1781674696128 [label="bn.weight
 (16)" fillcolor=lightblue]
			1781674696128 -> 1780361834112
		1780361834112 [label=AccumulateGrad]
			1780361833872 -> 1780361834064
		1781674692800 [label="bn.bias
 (16)" fillcolor=lightblue]
			1781674692800 -> 1780361833872
		1780361833872 [label=AccumulateGrad]
			1780361833968 -> 1780361833776
		1781675714944 [label="layer1.0.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675714944 -> 1780361833968
		1780361833968 [label=AccumulateGrad]
			1780361833728 -> 1780361833680
		1781675714880 [label="layer1.0.bn1.weight
 (16)" fillcolor=lightblue]
			1781675714880 -> 1780361833728
		1780361833728 [label=AccumulateGrad]
			1780361833584 -> 1780361833680
		1781675715200 [label="layer1.0.bn1.bias
 (16)" fillcolor=lightblue]
			1781675715200 -> 1780361833584
		1780361833584 [label=AccumulateGrad]
			1780361833488 -> 1780361833344
		1781675715776 [label="layer1.0.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675715776 -> 1780361833488
		1780361833488 [label=AccumulateGrad]
			1780361833296 -> 1780361833200
		1781675715008 [label="layer1.0.bn2.weight
 (16)" fillcolor=lightblue]
			1781675715008 -> 1780361833296
		1780361833296 [label=AccumulateGrad]
			1780361833248 -> 1780361833200
		1781675715840 [label="layer1.0.bn2.bias
 (16)" fillcolor=lightblue]
			1781675715840 -> 1780361833248
		1780361833248 [label=AccumulateGrad]
			1780361833152 -> 1780361833104
			1780361833008 -> 1780361832816
		1781570457984 [label="layer1.1.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781570457984 -> 1780361833008
		1780361833008 [label=AccumulateGrad]
			1780361832768 -> 1780361832720
		1781570457792 [label="layer1.1.bn1.weight
 (16)" fillcolor=lightblue]
			1781570457792 -> 1780361832768
		1780361832768 [label=AccumulateGrad]
			1780361832624 -> 1780361832720
		1781570458240 [label="layer1.1.bn1.bias
 (16)" fillcolor=lightblue]
			1781570458240 -> 1780361832624
		1780361832624 [label=AccumulateGrad]
			1780361832528 -> 1780361832384
		1781570459136 [label="layer1.1.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781570459136 -> 1780361832528
		1780361832528 [label=AccumulateGrad]
			1780361832336 -> 1780361832240
		1781570458048 [label="layer1.1.bn2.weight
 (16)" fillcolor=lightblue]
			1781570458048 -> 1780361832336
		1780361832336 [label=AccumulateGrad]
			1780361832288 -> 1780361832240
		1781570459200 [label="layer1.1.bn2.bias
 (16)" fillcolor=lightblue]
			1781570459200 -> 1780361832288
		1780361832288 [label=AccumulateGrad]
			1780361832192 -> 1780361832144
			1780361832048 -> 1780361831856
		1781570460224 [label="layer1.2.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781570460224 -> 1780361832048
		1780361832048 [label=AccumulateGrad]
			1780361831808 -> 1780361831760
		1781570460032 [label="layer1.2.bn1.weight
 (16)" fillcolor=lightblue]
			1781570460032 -> 1780361831808
		1780361831808 [label=AccumulateGrad]
			1780361831664 -> 1780361831760
		1781570460480 [label="layer1.2.bn1.bias
 (16)" fillcolor=lightblue]
			1781570460480 -> 1780361831664
		1780361831664 [label=AccumulateGrad]
			1780361831568 -> 1780361831424
		1781570461376 [label="layer1.2.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781570461376 -> 1780361831568
		1780361831568 [label=AccumulateGrad]
			1780361831376 -> 1780361831280
		1781570460288 [label="layer1.2.bn2.weight
 (16)" fillcolor=lightblue]
			1781570460288 -> 1780361831376
		1780361831376 [label=AccumulateGrad]
			1780361831328 -> 1780361831280
		1781570461440 [label="layer1.2.bn2.bias
 (16)" fillcolor=lightblue]
			1781570461440 -> 1780361831328
		1780361831328 [label=AccumulateGrad]
			1780361831232 -> 1780361831184
			1780361831088 -> 1780361830896
		1781570499392 [label="layer1.3.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781570499392 -> 1780361831088
		1780361831088 [label=AccumulateGrad]
			1780361830848 -> 1780361830800
		1781570499200 [label="layer1.3.bn1.weight
 (16)" fillcolor=lightblue]
			1781570499200 -> 1780361830848
		1780361830848 [label=AccumulateGrad]
			1780361830704 -> 1780361830800
		1781570499648 [label="layer1.3.bn1.bias
 (16)" fillcolor=lightblue]
			1781570499648 -> 1780361830704
		1780361830704 [label=AccumulateGrad]
			1780361830608 -> 1781571383248
		1781570500544 [label="layer1.3.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781570500544 -> 1780361830608
		1780361830608 [label=AccumulateGrad]
			1781571383200 -> 1781571383152
		1781570499456 [label="layer1.3.bn2.weight
 (16)" fillcolor=lightblue]
			1781570499456 -> 1781571383200
		1781571383200 [label=AccumulateGrad]
			1780361830464 -> 1781571383152
		1781570500608 [label="layer1.3.bn2.bias
 (16)" fillcolor=lightblue]
			1781570500608 -> 1780361830464
		1780361830464 [label=AccumulateGrad]
			1781571383104 -> 1781571383056
			1781571382960 -> 1781571382768
		1781570501632 [label="layer1.4.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781570501632 -> 1781571382960
		1781571382960 [label=AccumulateGrad]
			1781571382720 -> 1781571382672
		1781570501440 [label="layer1.4.bn1.weight
 (16)" fillcolor=lightblue]
			1781570501440 -> 1781571382720
		1781571382720 [label=AccumulateGrad]
			1781571382576 -> 1781571382672
		1781570501888 [label="layer1.4.bn1.bias
 (16)" fillcolor=lightblue]
			1781570501888 -> 1781571382576
		1781571382576 [label=AccumulateGrad]
			1781571382480 -> 1781571382336
		1781675790528 [label="layer1.4.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675790528 -> 1781571382480
		1781571382480 [label=AccumulateGrad]
			1781571382288 -> 1781571382192
		1781675790464 [label="layer1.4.bn2.weight
 (16)" fillcolor=lightblue]
			1781675790464 -> 1781571382288
		1781571382288 [label=AccumulateGrad]
			1781571382240 -> 1781571382192
		1781675790592 [label="layer1.4.bn2.bias
 (16)" fillcolor=lightblue]
			1781675790592 -> 1781571382240
		1781571382240 [label=AccumulateGrad]
			1781571382144 -> 1781571382096
			1781571382000 -> 1781571381808
		1781675791680 [label="layer1.5.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675791680 -> 1781571382000
		1781571382000 [label=AccumulateGrad]
			1781571381760 -> 1781571381712
		1781675791488 [label="layer1.5.bn1.weight
 (16)" fillcolor=lightblue]
			1781675791488 -> 1781571381760
		1781571381760 [label=AccumulateGrad]
			1781571381616 -> 1781571381712
		1781675791936 [label="layer1.5.bn1.bias
 (16)" fillcolor=lightblue]
			1781675791936 -> 1781571381616
		1781571381616 [label=AccumulateGrad]
			1781571381520 -> 1781571381376
		1781675792832 [label="layer1.5.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675792832 -> 1781571381520
		1781571381520 [label=AccumulateGrad]
			1781571381328 -> 1781571381232
		1781675791744 [label="layer1.5.bn2.weight
 (16)" fillcolor=lightblue]
			1781675791744 -> 1781571381328
		1781571381328 [label=AccumulateGrad]
			1781571381280 -> 1781571381232
		1781675792896 [label="layer1.5.bn2.bias
 (16)" fillcolor=lightblue]
			1781675792896 -> 1781571381280
		1781571381280 [label=AccumulateGrad]
			1781571381184 -> 1781571381136
			1781571381040 -> 1781571380848
		1781675793920 [label="layer1.6.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675793920 -> 1781571381040
		1781571381040 [label=AccumulateGrad]
			1781571380800 -> 1781571380752
		1781675793728 [label="layer1.6.bn1.weight
 (16)" fillcolor=lightblue]
			1781675793728 -> 1781571380800
		1781571380800 [label=AccumulateGrad]
			1781571380656 -> 1781571380752
		1781675794176 [label="layer1.6.bn1.bias
 (16)" fillcolor=lightblue]
			1781675794176 -> 1781571380656
		1781571380656 [label=AccumulateGrad]
			1781571380560 -> 1781571380416
		1781675832000 [label="layer1.6.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675832000 -> 1781571380560
		1781571380560 [label=AccumulateGrad]
			1781571380368 -> 1781571380272
		1781675831936 [label="layer1.6.bn2.weight
 (16)" fillcolor=lightblue]
			1781675831936 -> 1781571380368
		1781571380368 [label=AccumulateGrad]
			1781571380320 -> 1781571380272
		1781675832064 [label="layer1.6.bn2.bias
 (16)" fillcolor=lightblue]
			1781675832064 -> 1781571380320
		1781571380320 [label=AccumulateGrad]
			1781571380224 -> 1781571380176
			1781571380080 -> 1781571379888
		1781675833088 [label="layer1.7.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675833088 -> 1781571380080
		1781571380080 [label=AccumulateGrad]
			1781571379840 -> 1781571379792
		1781675832896 [label="layer1.7.bn1.weight
 (16)" fillcolor=lightblue]
			1781675832896 -> 1781571379840
		1781571379840 [label=AccumulateGrad]
			1781571379696 -> 1781571379792
		1781675833344 [label="layer1.7.bn1.bias
 (16)" fillcolor=lightblue]
			1781675833344 -> 1781571379696
		1781571379696 [label=AccumulateGrad]
			1781571379600 -> 1781571379456
		1781675834240 [label="layer1.7.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675834240 -> 1781571379600
		1781571379600 [label=AccumulateGrad]
			1781571379408 -> 1781571407632
		1781675833152 [label="layer1.7.bn2.weight
 (16)" fillcolor=lightblue]
			1781675833152 -> 1781571379408
		1781571379408 [label=AccumulateGrad]
			1781571379360 -> 1781571407632
		1781675834304 [label="layer1.7.bn2.bias
 (16)" fillcolor=lightblue]
			1781675834304 -> 1781571379360
		1781571379360 [label=AccumulateGrad]
			1781571379312 -> 1781571407824
			1781571407728 -> 1781571407536
		1781675835328 [label="layer1.8.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675835328 -> 1781571407728
		1781571407728 [label=AccumulateGrad]
			1781571407488 -> 1781571407440
		1781675884608 [label="layer1.8.bn1.weight
 (16)" fillcolor=lightblue]
			1781675884608 -> 1781571407488
		1781571407488 [label=AccumulateGrad]
			1781571407344 -> 1781571407440
		1781675884800 [label="layer1.8.bn1.bias
 (16)" fillcolor=lightblue]
			1781675884800 -> 1781571407344
		1781571407344 [label=AccumulateGrad]
			1781571407248 -> 1781571407104
		1781675885696 [label="layer1.8.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675885696 -> 1781571407248
		1781571407248 [label=AccumulateGrad]
			1781571407056 -> 1781571406960
		1781675885632 [label="layer1.8.bn2.weight
 (16)" fillcolor=lightblue]
			1781675885632 -> 1781571407056
		1781571407056 [label=AccumulateGrad]
			1781571407008 -> 1781571406960
		1781675885760 [label="layer1.8.bn2.bias
 (16)" fillcolor=lightblue]
			1781675885760 -> 1781571407008
		1781571407008 [label=AccumulateGrad]
			1781571406912 -> 1781571406864
			1781571406768 -> 1781571406576
		1781675886720 [label="layer1.9.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675886720 -> 1781571406768
		1781571406768 [label=AccumulateGrad]
			1781571406528 -> 1781571406480
		1781675886528 [label="layer1.9.bn1.weight
 (16)" fillcolor=lightblue]
			1781675886528 -> 1781571406528
		1781571406528 [label=AccumulateGrad]
			1781571406384 -> 1781571406480
		1781675886976 [label="layer1.9.bn1.bias
 (16)" fillcolor=lightblue]
			1781675886976 -> 1781571406384
		1781571406384 [label=AccumulateGrad]
			1781571406288 -> 1781571406144
		1781675887872 [label="layer1.9.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675887872 -> 1781571406288
		1781571406288 [label=AccumulateGrad]
			1781571406096 -> 1781571406000
		1781675886784 [label="layer1.9.bn2.weight
 (16)" fillcolor=lightblue]
			1781675886784 -> 1781571406096
		1781571406096 [label=AccumulateGrad]
			1781571406048 -> 1781571406000
		1781675887936 [label="layer1.9.bn2.bias
 (16)" fillcolor=lightblue]
			1781675887936 -> 1781571406048
		1781571406048 [label=AccumulateGrad]
			1781571405952 -> 1781571405904
			1781571405808 -> 1781571405616
		1781675917696 [label="layer1.10.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675917696 -> 1781571405808
		1781571405808 [label=AccumulateGrad]
			1781571405568 -> 1781571405520
		1781675917504 [label="layer1.10.bn1.weight
 (16)" fillcolor=lightblue]
			1781675917504 -> 1781571405568
		1781571405568 [label=AccumulateGrad]
			1781571405424 -> 1781571405520
		1781675917952 [label="layer1.10.bn1.bias
 (16)" fillcolor=lightblue]
			1781675917952 -> 1781571405424
		1781571405424 [label=AccumulateGrad]
			1781571405328 -> 1781571405184
		1781675918848 [label="layer1.10.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675918848 -> 1781571405328
		1781571405328 [label=AccumulateGrad]
			1781571405136 -> 1781571405040
		1781675917760 [label="layer1.10.bn2.weight
 (16)" fillcolor=lightblue]
			1781675917760 -> 1781571405136
		1781571405136 [label=AccumulateGrad]
			1781571405088 -> 1781571405040
		1781675918912 [label="layer1.10.bn2.bias
 (16)" fillcolor=lightblue]
			1781675918912 -> 1781571405088
		1781571405088 [label=AccumulateGrad]
			1781571404992 -> 1781571404944
			1781571404848 -> 1781571404656
		1781675919936 [label="layer1.11.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675919936 -> 1781571404848
		1781571404848 [label=AccumulateGrad]
			1781571404608 -> 1781571404560
		1781675919744 [label="layer1.11.bn1.weight
 (16)" fillcolor=lightblue]
			1781675919744 -> 1781571404608
		1781571404608 [label=AccumulateGrad]
			1781571404464 -> 1781571404560
		1781675920192 [label="layer1.11.bn1.bias
 (16)" fillcolor=lightblue]
			1781675920192 -> 1781571404464
		1781571404464 [label=AccumulateGrad]
			1781571404368 -> 1781571404224
		1781675921088 [label="layer1.11.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675921088 -> 1781571404368
		1781571404368 [label=AccumulateGrad]
			1781571404176 -> 1781571404080
		1781675920000 [label="layer1.11.bn2.weight
 (16)" fillcolor=lightblue]
			1781675920000 -> 1781571404176
		1781571404176 [label=AccumulateGrad]
			1781571404128 -> 1781571404080
		1781675921152 [label="layer1.11.bn2.bias
 (16)" fillcolor=lightblue]
			1781675921152 -> 1781571404128
		1781571404128 [label=AccumulateGrad]
			1781571404032 -> 1781571403984
			1781571420112 -> 1781571420016
		1781675955008 [label="layer1.12.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675955008 -> 1781571420112
		1781571420112 [label=AccumulateGrad]
			1781571419968 -> 1781571419920
		1781675954816 [label="layer1.12.bn1.weight
 (16)" fillcolor=lightblue]
			1781675954816 -> 1781571419968
		1781571419968 [label=AccumulateGrad]
			1781571419824 -> 1781571419920
		1781675955264 [label="layer1.12.bn1.bias
 (16)" fillcolor=lightblue]
			1781675955264 -> 1781571419824
		1781571419824 [label=AccumulateGrad]
			1781571419728 -> 1781571419584
		1781675956160 [label="layer1.12.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675956160 -> 1781571419728
		1781571419728 [label=AccumulateGrad]
			1781571419536 -> 1781571419440
		1781675955072 [label="layer1.12.bn2.weight
 (16)" fillcolor=lightblue]
			1781675955072 -> 1781571419536
		1781571419536 [label=AccumulateGrad]
			1781571419488 -> 1781571419440
		1781675956224 [label="layer1.12.bn2.bias
 (16)" fillcolor=lightblue]
			1781675956224 -> 1781571419488
		1781571419488 [label=AccumulateGrad]
			1781571419392 -> 1781571419344
			1781571419248 -> 1781571419056
		1781675957248 [label="layer1.13.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675957248 -> 1781571419248
		1781571419248 [label=AccumulateGrad]
			1781571419008 -> 1781571418960
		1781675957056 [label="layer1.13.bn1.weight
 (16)" fillcolor=lightblue]
			1781675957056 -> 1781571419008
		1781571419008 [label=AccumulateGrad]
			1781571418864 -> 1781571418960
		1781675957504 [label="layer1.13.bn1.bias
 (16)" fillcolor=lightblue]
			1781675957504 -> 1781571418864
		1781571418864 [label=AccumulateGrad]
			1781571418768 -> 1781571418624
		1781675995328 [label="layer1.13.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675995328 -> 1781571418768
		1781571418768 [label=AccumulateGrad]
			1781571418576 -> 1781571418480
		1781675995264 [label="layer1.13.bn2.weight
 (16)" fillcolor=lightblue]
			1781675995264 -> 1781571418576
		1781571418576 [label=AccumulateGrad]
			1781571418528 -> 1781571418480
		1781675995392 [label="layer1.13.bn2.bias
 (16)" fillcolor=lightblue]
			1781675995392 -> 1781571418528
		1781571418528 [label=AccumulateGrad]
			1781571418432 -> 1781571418384
			1781571418288 -> 1781571418096
		1781675996416 [label="layer1.14.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675996416 -> 1781571418288
		1781571418288 [label=AccumulateGrad]
			1781571418048 -> 1781571418000
		1781675996224 [label="layer1.14.bn1.weight
 (16)" fillcolor=lightblue]
			1781675996224 -> 1781571418048
		1781571418048 [label=AccumulateGrad]
			1781571417904 -> 1781571418000
		1781675996672 [label="layer1.14.bn1.bias
 (16)" fillcolor=lightblue]
			1781675996672 -> 1781571417904
		1781571417904 [label=AccumulateGrad]
			1781571417808 -> 1781571417664
		1781675997568 [label="layer1.14.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675997568 -> 1781571417808
		1781571417808 [label=AccumulateGrad]
			1781571417616 -> 1781571417520
		1781675996480 [label="layer1.14.bn2.weight
 (16)" fillcolor=lightblue]
			1781675996480 -> 1781571417616
		1781571417616 [label=AccumulateGrad]
			1781571417568 -> 1781571417520
		1781675997632 [label="layer1.14.bn2.bias
 (16)" fillcolor=lightblue]
			1781675997632 -> 1781571417568
		1781571417568 [label=AccumulateGrad]
			1781571417472 -> 1781571417424
			1781571417328 -> 1781571417136
		1781675998656 [label="layer1.15.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675998656 -> 1781571417328
		1781571417328 [label=AccumulateGrad]
			1781571417088 -> 1781571417040
		1781675998464 [label="layer1.15.bn1.weight
 (16)" fillcolor=lightblue]
			1781675998464 -> 1781571417088
		1781571417088 [label=AccumulateGrad]
			1781571416944 -> 1781571417040
		1781675998912 [label="layer1.15.bn1.bias
 (16)" fillcolor=lightblue]
			1781675998912 -> 1781571416944
		1781571416944 [label=AccumulateGrad]
			1781571416848 -> 1781571416704
		1781676028544 [label="layer1.15.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781676028544 -> 1781571416848
		1781571416848 [label=AccumulateGrad]
			1781571416656 -> 1781571416560
		1781676028480 [label="layer1.15.bn2.weight
 (16)" fillcolor=lightblue]
			1781676028480 -> 1781571416656
		1781571416656 [label=AccumulateGrad]
			1781571416608 -> 1781571416560
		1781676028608 [label="layer1.15.bn2.bias
 (16)" fillcolor=lightblue]
			1781676028608 -> 1781571416608
		1781571416608 [label=AccumulateGrad]
			1781571416512 -> 1781571416464
			1781571416368 -> 1781571444592
		1781676029632 [label="layer1.16.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781676029632 -> 1781571416368
		1781571416368 [label=AccumulateGrad]
			1781571416176 -> 1781571444688
		1781676029440 [label="layer1.16.bn1.weight
 (16)" fillcolor=lightblue]
			1781676029440 -> 1781571416176
		1781571416176 [label=AccumulateGrad]
			1781571416128 -> 1781571444688
		1781676029888 [label="layer1.16.bn1.bias
 (16)" fillcolor=lightblue]
			1781676029888 -> 1781571416128
		1781571416128 [label=AccumulateGrad]
			1781571444496 -> 1781571444352
		1781676030784 [label="layer1.16.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781676030784 -> 1781571444496
		1781571444496 [label=AccumulateGrad]
			1781571444304 -> 1781571444208
		1781676029696 [label="layer1.16.bn2.weight
 (16)" fillcolor=lightblue]
			1781676029696 -> 1781571444304
		1781571444304 [label=AccumulateGrad]
			1781571444256 -> 1781571444208
		1781676030848 [label="layer1.16.bn2.bias
 (16)" fillcolor=lightblue]
			1781676030848 -> 1781571444256
		1781571444256 [label=AccumulateGrad]
			1781571444160 -> 1781571444112
			1781571444016 -> 1781571443824
		1781676031872 [label="layer1.17.conv1.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781676031872 -> 1781571444016
		1781571444016 [label=AccumulateGrad]
			1781571443776 -> 1781571443728
		1781675540544 [label="layer1.17.bn1.weight
 (16)" fillcolor=lightblue]
			1781675540544 -> 1781571443776
		1781571443776 [label=AccumulateGrad]
			1781571443632 -> 1781571443728
		1781675540672 [label="layer1.17.bn1.bias
 (16)" fillcolor=lightblue]
			1781675540672 -> 1781571443632
		1781571443632 [label=AccumulateGrad]
			1781571443536 -> 1781571443392
		1781675541568 [label="layer1.17.conv2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
			1781675541568 -> 1781571443536
		1781571443536 [label=AccumulateGrad]
			1781571443344 -> 1781571443248
		1781675541504 [label="layer1.17.bn2.weight
 (16)" fillcolor=lightblue]
			1781675541504 -> 1781571443344
		1781571443344 [label=AccumulateGrad]
			1781571443296 -> 1781571443248
		1781675541632 [label="layer1.17.bn2.bias
 (16)" fillcolor=lightblue]
			1781675541632 -> 1781571443296
		1781571443296 [label=AccumulateGrad]
			1781571443200 -> 1781571443152
			1781571442960 -> 1781571442816
		1781675569664 [label="layer2.0.conv1.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
			1781675569664 -> 1781571442960
		1781571442960 [label=AccumulateGrad]
			1781571442768 -> 1781571442720
		1781675569472 [label="layer2.0.bn1.weight
 (32)" fillcolor=lightblue]
			1781675569472 -> 1781571442768
		1781571442768 [label=AccumulateGrad]
			1781571442624 -> 1781571442720
		1781675569920 [label="layer2.0.bn1.bias
 (32)" fillcolor=lightblue]
			1781675569920 -> 1781571442624
		1781571442624 [label=AccumulateGrad]
			1781571442528 -> 1781571442384
		1781675570816 [label="layer2.0.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675570816 -> 1781571442528
		1781571442528 [label=AccumulateGrad]
			1781571442336 -> 1781571442240
		1781675569728 [label="layer2.0.bn2.weight
 (32)" fillcolor=lightblue]
			1781675569728 -> 1781571442336
		1781571442336 [label=AccumulateGrad]
			1781571442288 -> 1781571442240
		1781675570880 [label="layer2.0.bn2.bias
 (32)" fillcolor=lightblue]
			1781675570880 -> 1781571442288
		1781571442288 [label=AccumulateGrad]
			1781571442192 -> 1781571442144
		1781571442192 [label=CudnnBatchNormBackward]
			1781571442912 -> 1781571442192
		1781571442912 [label=CudnnConvolutionBackward]
			1781571443008 -> 1781571442912
			1781571443056 -> 1781571442912
		1781675543680 [label="layer2.0.downsample.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
			1781675543680 -> 1781571443056
		1781571443056 [label=AccumulateGrad]
			1781571442480 -> 1781571442192
		1781675543616 [label="layer2.0.downsample.1.weight
 (32)" fillcolor=lightblue]
			1781675543616 -> 1781571442480
		1781571442480 [label=AccumulateGrad]
			1781571442432 -> 1781571442192
		1781675543744 [label="layer2.0.downsample.1.bias
 (32)" fillcolor=lightblue]
			1781675543744 -> 1781571442432
		1781571442432 [label=AccumulateGrad]
			1781571442048 -> 1781571441856
		1781675571904 [label="layer2.1.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675571904 -> 1781571442048
		1781571442048 [label=AccumulateGrad]
			1781571441808 -> 1781571441760
		1781675571712 [label="layer2.1.bn1.weight
 (32)" fillcolor=lightblue]
			1781675571712 -> 1781571441808
		1781571441808 [label=AccumulateGrad]
			1781571441664 -> 1781571441760
		1781675572160 [label="layer2.1.bn1.bias
 (32)" fillcolor=lightblue]
			1781675572160 -> 1781571441664
		1781571441664 [label=AccumulateGrad]
			1781571441568 -> 1781571441424
		1781675573056 [label="layer2.1.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675573056 -> 1781571441568
		1781571441568 [label=AccumulateGrad]
			1781571441376 -> 1781571441280
		1781675571968 [label="layer2.1.bn2.weight
 (32)" fillcolor=lightblue]
			1781675571968 -> 1781571441376
		1781571441376 [label=AccumulateGrad]
			1781571441328 -> 1781571441280
		1781675614272 [label="layer2.1.bn2.bias
 (32)" fillcolor=lightblue]
			1781675614272 -> 1781571441328
		1781571441328 [label=AccumulateGrad]
			1781571441232 -> 1781571441184
			1781571441088 -> 1781571440896
		1781675615168 [label="layer2.2.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675615168 -> 1781571441088
		1781571441088 [label=AccumulateGrad]
			1781571440848 -> 1781571440800
		1781675614976 [label="layer2.2.bn1.weight
 (32)" fillcolor=lightblue]
			1781675614976 -> 1781571440848
		1781571440848 [label=AccumulateGrad]
			1781571440704 -> 1781571440800
		1781675615424 [label="layer2.2.bn1.bias
 (32)" fillcolor=lightblue]
			1781675615424 -> 1781571440704
		1781571440704 [label=AccumulateGrad]
			1781571485600 -> 1781571485456
		1781675616320 [label="layer2.2.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675616320 -> 1781571485600
		1781571485600 [label=AccumulateGrad]
			1781571485408 -> 1781571485312
		1781675615232 [label="layer2.2.bn2.weight
 (32)" fillcolor=lightblue]
			1781675615232 -> 1781571485408
		1781571485408 [label=AccumulateGrad]
			1781571485360 -> 1781571485312
		1781675616384 [label="layer2.2.bn2.bias
 (32)" fillcolor=lightblue]
			1781675616384 -> 1781571485360
		1781571485360 [label=AccumulateGrad]
			1781571485264 -> 1781571485216
			1781571485120 -> 1781571484928
		1781675617408 [label="layer2.3.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675617408 -> 1781571485120
		1781571485120 [label=AccumulateGrad]
			1781571484880 -> 1781571484832
		1781675617216 [label="layer2.3.bn1.weight
 (32)" fillcolor=lightblue]
			1781675617216 -> 1781571484880
		1781571484880 [label=AccumulateGrad]
			1781571484736 -> 1781571484832
		1781675617664 [label="layer2.3.bn1.bias
 (32)" fillcolor=lightblue]
			1781675617664 -> 1781571484736
		1781571484736 [label=AccumulateGrad]
			1781571484640 -> 1781571484496
		1781675655488 [label="layer2.3.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675655488 -> 1781571484640
		1781571484640 [label=AccumulateGrad]
			1781571484448 -> 1781571484352
		1781675655424 [label="layer2.3.bn2.weight
 (32)" fillcolor=lightblue]
			1781675655424 -> 1781571484448
		1781571484448 [label=AccumulateGrad]
			1781571484400 -> 1781571484352
		1781675655552 [label="layer2.3.bn2.bias
 (32)" fillcolor=lightblue]
			1781675655552 -> 1781571484400
		1781571484400 [label=AccumulateGrad]
			1781571484304 -> 1781571484256
			1781571484160 -> 1781571483968
		1781675656576 [label="layer2.4.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675656576 -> 1781571484160
		1781571484160 [label=AccumulateGrad]
			1781571483920 -> 1781571483872
		1781675656384 [label="layer2.4.bn1.weight
 (32)" fillcolor=lightblue]
			1781675656384 -> 1781571483920
		1781571483920 [label=AccumulateGrad]
			1781571483776 -> 1781571483872
		1781675656832 [label="layer2.4.bn1.bias
 (32)" fillcolor=lightblue]
			1781675656832 -> 1781571483776
		1781571483776 [label=AccumulateGrad]
			1781571483680 -> 1781571483536
		1781675657728 [label="layer2.4.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675657728 -> 1781571483680
		1781571483680 [label=AccumulateGrad]
			1781571483488 -> 1781571483392
		1781675656640 [label="layer2.4.bn2.weight
 (32)" fillcolor=lightblue]
			1781675656640 -> 1781571483488
		1781571483488 [label=AccumulateGrad]
			1781571483440 -> 1781571483392
		1781675657792 [label="layer2.4.bn2.bias
 (32)" fillcolor=lightblue]
			1781675657792 -> 1781571483440
		1781571483440 [label=AccumulateGrad]
			1781571483344 -> 1781571483296
			1781571483200 -> 1781571483008
		1781675658880 [label="layer2.5.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675658880 -> 1781571483200
		1781571483200 [label=AccumulateGrad]
			1781571482960 -> 1781571482912
		1781675658688 [label="layer2.5.bn1.weight
 (32)" fillcolor=lightblue]
			1781675658688 -> 1781571482960
		1781571482960 [label=AccumulateGrad]
			1781571482816 -> 1781571482912
		1781675688000 [label="layer2.5.bn1.bias
 (32)" fillcolor=lightblue]
			1781675688000 -> 1781571482816
		1781571482816 [label=AccumulateGrad]
			1781571482720 -> 1781571482576
		1781675688768 [label="layer2.5.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675688768 -> 1781571482720
		1781571482720 [label=AccumulateGrad]
			1781571482528 -> 1781571482432
		1781675688704 [label="layer2.5.bn2.weight
 (32)" fillcolor=lightblue]
			1781675688704 -> 1781571482528
		1781571482528 [label=AccumulateGrad]
			1781571482480 -> 1781571482432
		1781675688832 [label="layer2.5.bn2.bias
 (32)" fillcolor=lightblue]
			1781675688832 -> 1781571482480
		1781571482480 [label=AccumulateGrad]
			1781571482384 -> 1781571482336
			1781571482240 -> 1781571482048
		1781675689856 [label="layer2.6.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675689856 -> 1781571482240
		1781571482240 [label=AccumulateGrad]
			1781571482000 -> 1781571481952
		1781675689664 [label="layer2.6.bn1.weight
 (32)" fillcolor=lightblue]
			1781675689664 -> 1781571482000
		1781571482000 [label=AccumulateGrad]
			1781571481856 -> 1781571481952
		1781675690112 [label="layer2.6.bn1.bias
 (32)" fillcolor=lightblue]
			1781675690112 -> 1781571481856
		1781571481856 [label=AccumulateGrad]
			1781571481760 -> 1781571477456
		1781675691008 [label="layer2.6.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675691008 -> 1781571481760
		1781571481760 [label=AccumulateGrad]
			1781571477408 -> 1781571477312
		1781675689920 [label="layer2.6.bn2.weight
 (32)" fillcolor=lightblue]
			1781675689920 -> 1781571477408
		1781571477408 [label=AccumulateGrad]
			1781571477360 -> 1781571477312
		1781675691072 [label="layer2.6.bn2.bias
 (32)" fillcolor=lightblue]
			1781675691072 -> 1781571477360
		1781571477360 [label=AccumulateGrad]
			1781571477264 -> 1781571477216
			1781571477120 -> 1781571476928
		1781675737216 [label="layer2.7.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675737216 -> 1781571477120
		1781571477120 [label=AccumulateGrad]
			1781571476880 -> 1781571476832
		1781675737280 [label="layer2.7.bn1.weight
 (32)" fillcolor=lightblue]
			1781675737280 -> 1781571476880
		1781571476880 [label=AccumulateGrad]
			1781571476736 -> 1781571476832
		1781675737472 [label="layer2.7.bn1.bias
 (32)" fillcolor=lightblue]
			1781675737472 -> 1781571476736
		1781571476736 [label=AccumulateGrad]
			1781571476640 -> 1781571476496
		1781675738368 [label="layer2.7.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675738368 -> 1781571476640
		1781571476640 [label=AccumulateGrad]
			1781571476448 -> 1781571476352
		1781675738304 [label="layer2.7.bn2.weight
 (32)" fillcolor=lightblue]
			1781675738304 -> 1781571476448
		1781571476448 [label=AccumulateGrad]
			1781571476400 -> 1781571476352
		1781675738432 [label="layer2.7.bn2.bias
 (32)" fillcolor=lightblue]
			1781675738432 -> 1781571476400
		1781571476400 [label=AccumulateGrad]
			1781571476304 -> 1781571476256
			1781571476160 -> 1781571475968
		1781675739456 [label="layer2.8.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675739456 -> 1781571476160
		1781571476160 [label=AccumulateGrad]
			1781571475920 -> 1781571475872
		1781675739264 [label="layer2.8.bn1.weight
 (32)" fillcolor=lightblue]
			1781675739264 -> 1781571475920
		1781571475920 [label=AccumulateGrad]
			1781571475776 -> 1781571475872
		1781675739712 [label="layer2.8.bn1.bias
 (32)" fillcolor=lightblue]
			1781675739712 -> 1781571475776
		1781571475776 [label=AccumulateGrad]
			1781571475680 -> 1781571475536
		1781675740608 [label="layer2.8.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675740608 -> 1781571475680
		1781571475680 [label=AccumulateGrad]
			1781571475488 -> 1781571475392
		1781675739520 [label="layer2.8.bn2.weight
 (32)" fillcolor=lightblue]
			1781675739520 -> 1781571475488
		1781571475488 [label=AccumulateGrad]
			1781571475440 -> 1781571475392
		1781675740672 [label="layer2.8.bn2.bias
 (32)" fillcolor=lightblue]
			1781675740672 -> 1781571475440
		1781571475440 [label=AccumulateGrad]
			1781571475344 -> 1781571475296
			1781571475200 -> 1781571475008
		1781675774464 [label="layer2.9.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675774464 -> 1781571475200
		1781571475200 [label=AccumulateGrad]
			1781571474960 -> 1781571474912
		1781675774272 [label="layer2.9.bn1.weight
 (32)" fillcolor=lightblue]
			1781675774272 -> 1781571474960
		1781571474960 [label=AccumulateGrad]
			1781571474816 -> 1781571474912
		1781675774720 [label="layer2.9.bn1.bias
 (32)" fillcolor=lightblue]
			1781675774720 -> 1781571474816
		1781571474816 [label=AccumulateGrad]
			1781571474720 -> 1781571474576
		1781675775616 [label="layer2.9.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675775616 -> 1781571474720
		1781571474720 [label=AccumulateGrad]
			1781571474528 -> 1781571474432
		1781675774528 [label="layer2.9.bn2.weight
 (32)" fillcolor=lightblue]
			1781675774528 -> 1781571474528
		1781571474528 [label=AccumulateGrad]
			1781571474480 -> 1781571474432
		1781675775680 [label="layer2.9.bn2.bias
 (32)" fillcolor=lightblue]
			1781675775680 -> 1781571474480
		1781571474480 [label=AccumulateGrad]
			1781571474384 -> 1781571474336
			1781571474240 -> 1781571474048
		1781675776704 [label="layer2.10.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675776704 -> 1781571474240
		1781571474240 [label=AccumulateGrad]
			1781571474000 -> 1781571473952
		1781675776512 [label="layer2.10.bn1.weight
 (32)" fillcolor=lightblue]
			1781675776512 -> 1781571474000
		1781571474000 [label=AccumulateGrad]
			1781571473856 -> 1781571473952
		1781675776960 [label="layer2.10.bn1.bias
 (32)" fillcolor=lightblue]
			1781675776960 -> 1781571473856
		1781571473856 [label=AccumulateGrad]
			1781571473760 -> 1781571473616
		1781675777856 [label="layer2.10.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781675777856 -> 1781571473760
		1781571473760 [label=AccumulateGrad]
			1781571473568 -> 1785186344912
		1781675776768 [label="layer2.10.bn2.weight
 (32)" fillcolor=lightblue]
			1781675776768 -> 1781571473568
		1781571473568 [label=AccumulateGrad]
			1781571473520 -> 1785186344912
		1781674500160 [label="layer2.10.bn2.bias
 (32)" fillcolor=lightblue]
			1781674500160 -> 1781571473520
		1781571473520 [label=AccumulateGrad]
			1785186344672 -> 1785186344864
			1785186344768 -> 1785186344576
		1781674501056 [label="layer2.11.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674501056 -> 1785186344768
		1785186344768 [label=AccumulateGrad]
			1785186344528 -> 1785186344480
		1781674500864 [label="layer2.11.bn1.weight
 (32)" fillcolor=lightblue]
			1781674500864 -> 1785186344528
		1785186344528 [label=AccumulateGrad]
			1785186344384 -> 1785186344480
		1781674501312 [label="layer2.11.bn1.bias
 (32)" fillcolor=lightblue]
			1781674501312 -> 1785186344384
		1785186344384 [label=AccumulateGrad]
			1785186344288 -> 1785186344144
		1781674502208 [label="layer2.11.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674502208 -> 1785186344288
		1785186344288 [label=AccumulateGrad]
			1785186344096 -> 1785186344000
		1781674501120 [label="layer2.11.bn2.weight
 (32)" fillcolor=lightblue]
			1781674501120 -> 1785186344096
		1785186344096 [label=AccumulateGrad]
			1785186344048 -> 1785186344000
		1781674502272 [label="layer2.11.bn2.bias
 (32)" fillcolor=lightblue]
			1781674502272 -> 1785186344048
		1785186344048 [label=AccumulateGrad]
			1785186343952 -> 1785186343904
			1785186343808 -> 1785186343616
		1781674503296 [label="layer2.12.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674503296 -> 1785186343808
		1785186343808 [label=AccumulateGrad]
			1785186343568 -> 1785186343520
		1781674503104 [label="layer2.12.bn1.weight
 (32)" fillcolor=lightblue]
			1781674503104 -> 1785186343568
		1785186343568 [label=AccumulateGrad]
			1785186343424 -> 1785186343520
		1781674503552 [label="layer2.12.bn1.bias
 (32)" fillcolor=lightblue]
			1781674503552 -> 1785186343424
		1785186343424 [label=AccumulateGrad]
			1785186343328 -> 1785186343184
		1781674537280 [label="layer2.12.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674537280 -> 1785186343328
		1785186343328 [label=AccumulateGrad]
			1785186343136 -> 1785186343040
		1781674537216 [label="layer2.12.bn2.weight
 (32)" fillcolor=lightblue]
			1781674537216 -> 1785186343136
		1785186343136 [label=AccumulateGrad]
			1785186343088 -> 1785186343040
		1781674537344 [label="layer2.12.bn2.bias
 (32)" fillcolor=lightblue]
			1781674537344 -> 1785186343088
		1785186343088 [label=AccumulateGrad]
			1785186342992 -> 1785186342944
			1785186342848 -> 1785186342656
		1781674538368 [label="layer2.13.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674538368 -> 1785186342848
		1785186342848 [label=AccumulateGrad]
			1785186342608 -> 1785186342560
		1781674538176 [label="layer2.13.bn1.weight
 (32)" fillcolor=lightblue]
			1781674538176 -> 1785186342608
		1785186342608 [label=AccumulateGrad]
			1785186342464 -> 1785186342560
		1781674538624 [label="layer2.13.bn1.bias
 (32)" fillcolor=lightblue]
			1781674538624 -> 1785186342464
		1785186342464 [label=AccumulateGrad]
			1785186342368 -> 1785186342224
		1781674539520 [label="layer2.13.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674539520 -> 1785186342368
		1785186342368 [label=AccumulateGrad]
			1785186342176 -> 1785186342080
		1781674538432 [label="layer2.13.bn2.weight
 (32)" fillcolor=lightblue]
			1781674538432 -> 1785186342176
		1785186342176 [label=AccumulateGrad]
			1785186342128 -> 1785186342080
		1781674539584 [label="layer2.13.bn2.bias
 (32)" fillcolor=lightblue]
			1781674539584 -> 1785186342128
		1785186342128 [label=AccumulateGrad]
			1785186342032 -> 1785186341984
			1785186341888 -> 1785186341696
		1781674540608 [label="layer2.14.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674540608 -> 1785186341888
		1785186341888 [label=AccumulateGrad]
			1785186341648 -> 1785186341600
		1781674540416 [label="layer2.14.bn1.weight
 (32)" fillcolor=lightblue]
			1781674540416 -> 1785186341648
		1785186341648 [label=AccumulateGrad]
			1785186341504 -> 1785186341600
		1781674540864 [label="layer2.14.bn1.bias
 (32)" fillcolor=lightblue]
			1781674540864 -> 1785186341504
		1785186341504 [label=AccumulateGrad]
			1785186341408 -> 1785186341264
		1781674574592 [label="layer2.14.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674574592 -> 1785186341408
		1785186341408 [label=AccumulateGrad]
			1785186341216 -> 1785186341120
		1781674574528 [label="layer2.14.bn2.weight
 (32)" fillcolor=lightblue]
			1781674574528 -> 1785186341216
		1785186341216 [label=AccumulateGrad]
			1785186341168 -> 1785186341120
		1781674574656 [label="layer2.14.bn2.bias
 (32)" fillcolor=lightblue]
			1781674574656 -> 1785186341168
		1785186341168 [label=AccumulateGrad]
			1785186341072 -> 1785186341024
			1785186373584 -> 1785186373440
		1781674575680 [label="layer2.15.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674575680 -> 1785186373584
		1785186373584 [label=AccumulateGrad]
			1785186373392 -> 1785186373344
		1781674575488 [label="layer2.15.bn1.weight
 (32)" fillcolor=lightblue]
			1781674575488 -> 1785186373392
		1785186373392 [label=AccumulateGrad]
			1785186373248 -> 1785186373344
		1781674575936 [label="layer2.15.bn1.bias
 (32)" fillcolor=lightblue]
			1781674575936 -> 1785186373248
		1785186373248 [label=AccumulateGrad]
			1785186373152 -> 1785186373008
		1781674576832 [label="layer2.15.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674576832 -> 1785186373152
		1785186373152 [label=AccumulateGrad]
			1785186372960 -> 1785186372864
		1781674575744 [label="layer2.15.bn2.weight
 (32)" fillcolor=lightblue]
			1781674575744 -> 1785186372960
		1785186372960 [label=AccumulateGrad]
			1785186372912 -> 1785186372864
		1781674576896 [label="layer2.15.bn2.bias
 (32)" fillcolor=lightblue]
			1781674576896 -> 1785186372912
		1785186372912 [label=AccumulateGrad]
			1785186372816 -> 1785186372768
			1785186372672 -> 1785186372480
		1781674614848 [label="layer2.16.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674614848 -> 1785186372672
		1785186372672 [label=AccumulateGrad]
			1785186372432 -> 1785186372384
		1781674614912 [label="layer2.16.bn1.weight
 (32)" fillcolor=lightblue]
			1781674614912 -> 1785186372432
		1785186372432 [label=AccumulateGrad]
			1785186372288 -> 1785186372384
		1781674615104 [label="layer2.16.bn1.bias
 (32)" fillcolor=lightblue]
			1781674615104 -> 1785186372288
		1785186372288 [label=AccumulateGrad]
			1785186372192 -> 1785186372048
		1781674616000 [label="layer2.16.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674616000 -> 1785186372192
		1785186372192 [label=AccumulateGrad]
			1785186372000 -> 1785186371904
		1781674615936 [label="layer2.16.bn2.weight
 (32)" fillcolor=lightblue]
			1781674615936 -> 1785186372000
		1785186372000 [label=AccumulateGrad]
			1785186371952 -> 1785186371904
		1781674616064 [label="layer2.16.bn2.bias
 (32)" fillcolor=lightblue]
			1781674616064 -> 1785186371952
		1785186371952 [label=AccumulateGrad]
			1785186371856 -> 1785186371808
			1785186371712 -> 1785186371520
		1781674617152 [label="layer2.17.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674617152 -> 1785186371712
		1785186371712 [label=AccumulateGrad]
			1785186371472 -> 1785186371424
		1781674616960 [label="layer2.17.bn1.weight
 (32)" fillcolor=lightblue]
			1781674616960 -> 1785186371472
		1785186371472 [label=AccumulateGrad]
			1785186371328 -> 1785186371424
		1781674617408 [label="layer2.17.bn1.bias
 (32)" fillcolor=lightblue]
			1781674617408 -> 1785186371328
		1785186371328 [label=AccumulateGrad]
			1785186371232 -> 1785186371088
		1781674618304 [label="layer2.17.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
			1781674618304 -> 1785186371232
		1785186371232 [label=AccumulateGrad]
			1785186371040 -> 1785186370944
		1781674617216 [label="layer2.17.bn2.weight
 (32)" fillcolor=lightblue]
			1781674617216 -> 1785186371040
		1785186371040 [label=AccumulateGrad]
			1785186370992 -> 1785186370944
		1781674618368 [label="layer2.17.bn2.bias
 (32)" fillcolor=lightblue]
			1781674618368 -> 1785186370992
		1785186370992 [label=AccumulateGrad]
			1785186370896 -> 1785186370848
			1785186370656 -> 1785186370512
		1781674650560 [label="layer3.0.conv1.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
			1781674650560 -> 1785186370656
		1785186370656 [label=AccumulateGrad]
			1785186370464 -> 1785186370416
		1781674650368 [label="layer3.0.bn1.weight
 (64)" fillcolor=lightblue]
			1781674650368 -> 1785186370464
		1785186370464 [label=AccumulateGrad]
			1785186370320 -> 1785186370416
		1781674650816 [label="layer3.0.bn1.bias
 (64)" fillcolor=lightblue]
			1781674650816 -> 1785186370320
		1785186370320 [label=AccumulateGrad]
			1785186370224 -> 1785186370080
		1781674684544 [label="layer3.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674684544 -> 1785186370224
		1785186370224 [label=AccumulateGrad]
			1785186370032 -> 1785186369936
		1781674684480 [label="layer3.0.bn2.weight
 (64)" fillcolor=lightblue]
			1781674684480 -> 1785186370032
		1785186370032 [label=AccumulateGrad]
			1785186369984 -> 1785186369936
		1781674684608 [label="layer3.0.bn2.bias
 (64)" fillcolor=lightblue]
			1781674684608 -> 1785186369984
		1785186369984 [label=AccumulateGrad]
			1785186369888 -> 1785186369840
		1785186369888 [label=CudnnBatchNormBackward]
			1785186370608 -> 1785186369888
		1785186370608 [label=CudnnConvolutionBackward]
			1785186370704 -> 1785186370608
			1785186370752 -> 1785186370608
		1781674649216 [label="layer3.0.downsample.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
			1781674649216 -> 1785186370752
		1785186370752 [label=AccumulateGrad]
			1785186370176 -> 1785186369888
		1781674649152 [label="layer3.0.downsample.1.weight
 (64)" fillcolor=lightblue]
			1781674649152 -> 1785186370176
		1785186370176 [label=AccumulateGrad]
			1785186370128 -> 1785186369888
		1781674649280 [label="layer3.0.downsample.1.bias
 (64)" fillcolor=lightblue]
			1781674649280 -> 1785186370128
		1785186370128 [label=AccumulateGrad]
			1785186369744 -> 1785186361296
		1781674685632 [label="layer3.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674685632 -> 1785186369744
		1785186369744 [label=AccumulateGrad]
			1785186361248 -> 1785186361200
		1781674685440 [label="layer3.1.bn1.weight
 (64)" fillcolor=lightblue]
			1781674685440 -> 1785186361248
		1785186361248 [label=AccumulateGrad]
			1785186361104 -> 1785186361200
		1781674685888 [label="layer3.1.bn1.bias
 (64)" fillcolor=lightblue]
			1781674685888 -> 1785186361104
		1785186361104 [label=AccumulateGrad]
			1785186361008 -> 1785186360864
		1781674686784 [label="layer3.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674686784 -> 1785186361008
		1785186361008 [label=AccumulateGrad]
			1785186360816 -> 1785186360720
		1781674685696 [label="layer3.1.bn2.weight
 (64)" fillcolor=lightblue]
			1781674685696 -> 1785186360816
		1785186360816 [label=AccumulateGrad]
			1785186360768 -> 1785186360720
		1781674686848 [label="layer3.1.bn2.bias
 (64)" fillcolor=lightblue]
			1781674686848 -> 1785186360768
		1785186360768 [label=AccumulateGrad]
			1785186360672 -> 1785186360624
			1785186360528 -> 1785186360336
		1781674687872 [label="layer3.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674687872 -> 1785186360528
		1785186360528 [label=AccumulateGrad]
			1785186360288 -> 1785186360240
		1781674687680 [label="layer3.2.bn1.weight
 (64)" fillcolor=lightblue]
			1781674687680 -> 1785186360288
		1785186360288 [label=AccumulateGrad]
			1785186360144 -> 1785186360240
		1781674688128 [label="layer3.2.bn1.bias
 (64)" fillcolor=lightblue]
			1781674688128 -> 1785186360144
		1785186360144 [label=AccumulateGrad]
			1785186360048 -> 1785186359904
		1781674717760 [label="layer3.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674717760 -> 1785186360048
		1785186360048 [label=AccumulateGrad]
			1785186359856 -> 1785186359760
		1781674717696 [label="layer3.2.bn2.weight
 (64)" fillcolor=lightblue]
			1781674717696 -> 1785186359856
		1785186359856 [label=AccumulateGrad]
			1785186359808 -> 1785186359760
		1781674717824 [label="layer3.2.bn2.bias
 (64)" fillcolor=lightblue]
			1781674717824 -> 1785186359808
		1785186359808 [label=AccumulateGrad]
			1785186359712 -> 1785186359664
			1785186359568 -> 1785186359376
		1781674718848 [label="layer3.3.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674718848 -> 1785186359568
		1785186359568 [label=AccumulateGrad]
			1785186359328 -> 1785186359280
		1781674718656 [label="layer3.3.bn1.weight
 (64)" fillcolor=lightblue]
			1781674718656 -> 1785186359328
		1785186359328 [label=AccumulateGrad]
			1785186359184 -> 1785186359280
		1781674719104 [label="layer3.3.bn1.bias
 (64)" fillcolor=lightblue]
			1781674719104 -> 1785186359184
		1785186359184 [label=AccumulateGrad]
			1785186359088 -> 1785186358944
		1781674720000 [label="layer3.3.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674720000 -> 1785186359088
		1785186359088 [label=AccumulateGrad]
			1785186358896 -> 1785186358800
		1781674718912 [label="layer3.3.bn2.weight
 (64)" fillcolor=lightblue]
			1781674718912 -> 1785186358896
		1785186358896 [label=AccumulateGrad]
			1785186358848 -> 1785186358800
		1781674720064 [label="layer3.3.bn2.bias
 (64)" fillcolor=lightblue]
			1781674720064 -> 1785186358848
		1785186358848 [label=AccumulateGrad]
			1785186358752 -> 1785186358704
			1785186358608 -> 1785186358416
		1781674721088 [label="layer3.4.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781674721088 -> 1785186358608
		1785186358608 [label=AccumulateGrad]
			1785186358368 -> 1785186358320
		1781570752576 [label="layer3.4.bn1.weight
 (64)" fillcolor=lightblue]
			1781570752576 -> 1785186358368
		1785186358368 [label=AccumulateGrad]
			1785186358224 -> 1785186358320
		1781570752640 [label="layer3.4.bn1.bias
 (64)" fillcolor=lightblue]
			1781570752640 -> 1785186358224
		1785186358224 [label=AccumulateGrad]
			1785186358128 -> 1785186357984
		1781570753536 [label="layer3.4.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570753536 -> 1785186358128
		1785186358128 [label=AccumulateGrad]
			1785186357936 -> 1785186357840
		1781570753472 [label="layer3.4.bn2.weight
 (64)" fillcolor=lightblue]
			1781570753472 -> 1785186357936
		1785186357936 [label=AccumulateGrad]
			1785186357888 -> 1785186357840
		1781570753600 [label="layer3.4.bn2.bias
 (64)" fillcolor=lightblue]
			1781570753600 -> 1785186357888
		1785186357888 [label=AccumulateGrad]
			1785186357792 -> 1785186357744
			1785186357648 -> 1785186357456
		1781570754688 [label="layer3.5.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570754688 -> 1785186357648
		1785186357648 [label=AccumulateGrad]
			1785186357408 -> 1785186406352
		1781570754496 [label="layer3.5.bn1.weight
 (64)" fillcolor=lightblue]
			1781570754496 -> 1785186357408
		1785186357408 [label=AccumulateGrad]
			1785186357312 -> 1785186406352
		1781570754944 [label="layer3.5.bn1.bias
 (64)" fillcolor=lightblue]
			1781570754944 -> 1785186357312
		1785186357312 [label=AccumulateGrad]
			1785186406256 -> 1785186406112
		1781570755840 [label="layer3.5.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570755840 -> 1785186406256
		1785186406256 [label=AccumulateGrad]
			1785186406064 -> 1785186405968
		1781570754752 [label="layer3.5.bn2.weight
 (64)" fillcolor=lightblue]
			1781570754752 -> 1785186406064
		1785186406064 [label=AccumulateGrad]
			1785186406016 -> 1785186405968
		1781570755904 [label="layer3.5.bn2.bias
 (64)" fillcolor=lightblue]
			1781570755904 -> 1785186406016
		1785186406016 [label=AccumulateGrad]
			1785186405920 -> 1785186405872
			1785186405776 -> 1785186405584
		1781570793856 [label="layer3.6.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570793856 -> 1785186405776
		1785186405776 [label=AccumulateGrad]
			1785186405536 -> 1785186405488
		1781570793664 [label="layer3.6.bn1.weight
 (64)" fillcolor=lightblue]
			1781570793664 -> 1785186405536
		1785186405536 [label=AccumulateGrad]
			1785186405392 -> 1785186405488
		1781570794112 [label="layer3.6.bn1.bias
 (64)" fillcolor=lightblue]
			1781570794112 -> 1785186405392
		1785186405392 [label=AccumulateGrad]
			1785186405296 -> 1785186405152
		1781570795008 [label="layer3.6.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570795008 -> 1785186405296
		1785186405296 [label=AccumulateGrad]
			1785186405104 -> 1785186405008
		1781570793920 [label="layer3.6.bn2.weight
 (64)" fillcolor=lightblue]
			1781570793920 -> 1785186405104
		1785186405104 [label=AccumulateGrad]
			1785186405056 -> 1785186405008
		1781570795072 [label="layer3.6.bn2.bias
 (64)" fillcolor=lightblue]
			1781570795072 -> 1785186405056
		1785186405056 [label=AccumulateGrad]
			1785186404960 -> 1785186404912
			1785186404816 -> 1785186404624
		1781570796096 [label="layer3.7.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570796096 -> 1785186404816
		1785186404816 [label=AccumulateGrad]
			1785186404576 -> 1785186404528
		1781570795904 [label="layer3.7.bn1.weight
 (64)" fillcolor=lightblue]
			1781570795904 -> 1785186404576
		1785186404576 [label=AccumulateGrad]
			1785186404432 -> 1785186404528
		1781570796352 [label="layer3.7.bn1.bias
 (64)" fillcolor=lightblue]
			1781570796352 -> 1785186404432
		1785186404432 [label=AccumulateGrad]
			1785186404336 -> 1785186404192
		1781570797248 [label="layer3.7.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570797248 -> 1785186404336
		1785186404336 [label=AccumulateGrad]
			1785186404144 -> 1785186404048
		1781570796160 [label="layer3.7.bn2.weight
 (64)" fillcolor=lightblue]
			1781570796160 -> 1785186404144
		1785186404144 [label=AccumulateGrad]
			1785186404096 -> 1785186404048
		1781570797312 [label="layer3.7.bn2.bias
 (64)" fillcolor=lightblue]
			1781570797312 -> 1785186404096
		1785186404096 [label=AccumulateGrad]
			1785186404000 -> 1785186403952
			1785186403856 -> 1785186403664
		1781570827072 [label="layer3.8.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570827072 -> 1785186403856
		1785186403856 [label=AccumulateGrad]
			1785186403616 -> 1785186403568
		1781570826880 [label="layer3.8.bn1.weight
 (64)" fillcolor=lightblue]
			1781570826880 -> 1785186403616
		1785186403616 [label=AccumulateGrad]
			1785186403472 -> 1785186403568
		1781570827328 [label="layer3.8.bn1.bias
 (64)" fillcolor=lightblue]
			1781570827328 -> 1785186403472
		1785186403472 [label=AccumulateGrad]
			1785186403376 -> 1785186403232
		1781570828224 [label="layer3.8.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570828224 -> 1785186403376
		1785186403376 [label=AccumulateGrad]
			1785186403184 -> 1785186403088
		1781570827136 [label="layer3.8.bn2.weight
 (64)" fillcolor=lightblue]
			1781570827136 -> 1785186403184
		1785186403184 [label=AccumulateGrad]
			1785186403136 -> 1785186403088
		1781570828288 [label="layer3.8.bn2.bias
 (64)" fillcolor=lightblue]
			1781570828288 -> 1785186403136
		1785186403136 [label=AccumulateGrad]
			1785186403040 -> 1785186402992
			1785186402896 -> 1785186402704
		1781570829248 [label="layer3.9.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570829248 -> 1785186402896
		1785186402896 [label=AccumulateGrad]
			1785186402656 -> 1785186402608
		1781570829056 [label="layer3.9.bn1.weight
 (64)" fillcolor=lightblue]
			1781570829056 -> 1785186402656
		1785186402656 [label=AccumulateGrad]
			1785186402512 -> 1785186402608
		1781570829504 [label="layer3.9.bn1.bias
 (64)" fillcolor=lightblue]
			1781570829504 -> 1785186402512
		1785186402512 [label=AccumulateGrad]
			1785186402416 -> 1782065684384
		1781570867328 [label="layer3.9.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570867328 -> 1785186402416
		1785186402416 [label=AccumulateGrad]
			1782065684336 -> 1782065684240
		1781570867264 [label="layer3.9.bn2.weight
 (64)" fillcolor=lightblue]
			1781570867264 -> 1782065684336
		1782065684336 [label=AccumulateGrad]
			1782065684288 -> 1782065684240
		1781570867392 [label="layer3.9.bn2.bias
 (64)" fillcolor=lightblue]
			1781570867392 -> 1782065684288
		1782065684288 [label=AccumulateGrad]
			1782065684192 -> 1782065684144
			1782065684048 -> 1782065683856
		1781570868416 [label="layer3.10.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570868416 -> 1782065684048
		1782065684048 [label=AccumulateGrad]
			1782065683808 -> 1782065683760
		1781570868224 [label="layer3.10.bn1.weight
 (64)" fillcolor=lightblue]
			1781570868224 -> 1782065683808
		1782065683808 [label=AccumulateGrad]
			1782065683664 -> 1782065683760
		1781570868672 [label="layer3.10.bn1.bias
 (64)" fillcolor=lightblue]
			1781570868672 -> 1782065683664
		1782065683664 [label=AccumulateGrad]
			1782065683568 -> 1782065683424
		1781570869568 [label="layer3.10.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570869568 -> 1782065683568
		1782065683568 [label=AccumulateGrad]
			1782065683376 -> 1782065683280
		1781570868480 [label="layer3.10.bn2.weight
 (64)" fillcolor=lightblue]
			1781570868480 -> 1782065683376
		1782065683376 [label=AccumulateGrad]
			1782065683328 -> 1782065683280
		1781570869632 [label="layer3.10.bn2.bias
 (64)" fillcolor=lightblue]
			1781570869632 -> 1782065683328
		1782065683328 [label=AccumulateGrad]
			1782065683232 -> 1782065683184
			1782065683088 -> 1782065682896
		1781570870656 [label="layer3.11.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570870656 -> 1782065683088
		1782065683088 [label=AccumulateGrad]
			1782065682848 -> 1782065682800
		1781570870464 [label="layer3.11.bn1.weight
 (64)" fillcolor=lightblue]
			1781570870464 -> 1782065682848
		1782065682848 [label=AccumulateGrad]
			1782065682704 -> 1782065682800
		1781570870912 [label="layer3.11.bn1.bias
 (64)" fillcolor=lightblue]
			1781570870912 -> 1782065682704
		1782065682704 [label=AccumulateGrad]
			1782065682608 -> 1782065682464
		1781570904640 [label="layer3.11.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570904640 -> 1782065682608
		1782065682608 [label=AccumulateGrad]
			1782065682416 -> 1782065682320
		1781570904576 [label="layer3.11.bn2.weight
 (64)" fillcolor=lightblue]
			1781570904576 -> 1782065682416
		1782065682416 [label=AccumulateGrad]
			1782065682368 -> 1782065682320
		1781570904704 [label="layer3.11.bn2.bias
 (64)" fillcolor=lightblue]
			1781570904704 -> 1782065682368
		1782065682368 [label=AccumulateGrad]
			1782065682272 -> 1785186425968
			1785186425776 -> 1785186424384
		1781570905728 [label="layer3.12.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570905728 -> 1785186425776
		1785186425776 [label=AccumulateGrad]
			1785186424576 -> 1785186423040
		1781570905536 [label="layer3.12.bn1.weight
 (64)" fillcolor=lightblue]
			1781570905536 -> 1785186424576
		1785186424576 [label=AccumulateGrad]
			1785186423232 -> 1785186423040
		1781570905984 [label="layer3.12.bn1.bias
 (64)" fillcolor=lightblue]
			1781570905984 -> 1785186423232
		1785186423232 [label=AccumulateGrad]
			1785186423424 -> 1785186423712
		1781570906880 [label="layer3.12.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570906880 -> 1785186423424
		1785186423424 [label=AccumulateGrad]
			1785186423808 -> 1785186424000
		1781570905792 [label="layer3.12.bn2.weight
 (64)" fillcolor=lightblue]
			1781570905792 -> 1785186423808
		1785186423808 [label=AccumulateGrad]
			1785186423904 -> 1785186424000
		1781570906944 [label="layer3.12.bn2.bias
 (64)" fillcolor=lightblue]
			1781570906944 -> 1785186423904
		1785186423904 [label=AccumulateGrad]
			1785186425824 -> 1785186424144
			1785186424096 -> 1785186304544
		1781570907968 [label="layer3.13.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570907968 -> 1785186424096
		1785186424096 [label=AccumulateGrad]
			1785186306656 -> 1785186304208
		1781570949184 [label="layer3.13.bn1.weight
 (64)" fillcolor=lightblue]
			1781570949184 -> 1785186306656
		1785186306656 [label=AccumulateGrad]
			1785186424288 -> 1785186304208
		1781570949248 [label="layer3.13.bn1.bias
 (64)" fillcolor=lightblue]
			1781570949248 -> 1785186424288
		1785186424288 [label=AccumulateGrad]
			1785186307712 -> 1785186307952
		1781570950144 [label="layer3.13.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570950144 -> 1785186307712
		1785186307712 [label=AccumulateGrad]
			1785186308048 -> 1785186307808
		1781570950080 [label="layer3.13.bn2.weight
 (64)" fillcolor=lightblue]
			1781570950080 -> 1785186308048
		1785186308048 [label=AccumulateGrad]
			1785186307568 -> 1785186307808
		1781570950208 [label="layer3.13.bn2.bias
 (64)" fillcolor=lightblue]
			1781570950208 -> 1785186307568
		1785186307568 [label=AccumulateGrad]
			1785186307232 -> 1785186307664
			1785186024464 -> 1785186024992
		1781570951232 [label="layer3.14.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570951232 -> 1785186024464
		1785186024464 [label=AccumulateGrad]
			1785186024656 -> 1785186024560
		1781570951040 [label="layer3.14.bn1.weight
 (64)" fillcolor=lightblue]
			1781570951040 -> 1785186024656
		1785186024656 [label=AccumulateGrad]
			1785186024752 -> 1785186024560
		1781570951488 [label="layer3.14.bn1.bias
 (64)" fillcolor=lightblue]
			1781570951488 -> 1785186024752
		1785186024752 [label=AccumulateGrad]
			1785186024704 -> 1782065467104
		1781570952384 [label="layer3.14.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570952384 -> 1785186024704
		1785186024704 [label=AccumulateGrad]
			1785186025232 -> 1782065463888
		1781570951296 [label="layer3.14.bn2.weight
 (64)" fillcolor=lightblue]
			1781570951296 -> 1785186025232
		1785186025232 [label=AccumulateGrad]
			1785186025136 -> 1782065463888
		1781570952448 [label="layer3.14.bn2.bias
 (64)" fillcolor=lightblue]
			1781570952448 -> 1785186025136
		1785186025136 [label=AccumulateGrad]
			1782065463984 -> 1782065464080
			1782065464272 -> 1782065464656
		1781570986304 [label="layer3.15.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570986304 -> 1782065464272
		1782065464272 [label=AccumulateGrad]
			1782065464752 -> 1782065464848
		1781570986112 [label="layer3.15.bn1.weight
 (64)" fillcolor=lightblue]
			1781570986112 -> 1782065464752
		1782065464752 [label=AccumulateGrad]
			1782065465040 -> 1782065464848
		1781570986560 [label="layer3.15.bn1.bias
 (64)" fillcolor=lightblue]
			1781570986560 -> 1782065465040
		1782065465040 [label=AccumulateGrad]
			1782065465232 -> 1782065465520
		1781570987456 [label="layer3.15.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570987456 -> 1782065465232
		1782065465232 [label=AccumulateGrad]
			1782065465616 -> 1782065465808
		1781570986368 [label="layer3.15.bn2.weight
 (64)" fillcolor=lightblue]
			1781570986368 -> 1782065465616
		1782065465616 [label=AccumulateGrad]
			1782065465712 -> 1782065465808
		1781570987520 [label="layer3.15.bn2.bias
 (64)" fillcolor=lightblue]
			1781570987520 -> 1782065465712
		1782065465712 [label=AccumulateGrad]
			1782065465904 -> 1782065466000
			1782065466192 -> 1782065466576
		1781570988544 [label="layer3.16.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570988544 -> 1782065466192
		1782065466192 [label=AccumulateGrad]
			1782065466672 -> 1782065466768
		1781570988352 [label="layer3.16.bn1.weight
 (64)" fillcolor=lightblue]
			1781570988352 -> 1782065466672
		1782065466672 [label=AccumulateGrad]
			1782065466960 -> 1782065466768
		1781570988800 [label="layer3.16.bn1.bias
 (64)" fillcolor=lightblue]
			1781570988800 -> 1782065466960
		1782065466960 [label=AccumulateGrad]
			1782065463840 -> 1782065467248
		1781570989696 [label="layer3.16.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781570989696 -> 1782065463840
		1782065463840 [label=AccumulateGrad]
			1782065467296 -> 1785186373888
		1781570988608 [label="layer3.16.bn2.weight
 (64)" fillcolor=lightblue]
			1781570988608 -> 1782065467296
		1782065467296 [label=AccumulateGrad]
			1782065467344 -> 1785186373888
		1781570989760 [label="layer3.16.bn2.bias
 (64)" fillcolor=lightblue]
			1781570989760 -> 1782065467344
		1782065467344 [label=AccumulateGrad]
			1785186376672 -> 1785186374752
			1785186374944 -> 1785186376480
		1781571281664 [label="layer3.17.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781571281664 -> 1785186374944
		1785186374944 [label=AccumulateGrad]
			1785186375808 -> 1785186374464
		1781571281472 [label="layer3.17.bn1.weight
 (64)" fillcolor=lightblue]
			1781571281472 -> 1785186375808
		1785186375808 [label=AccumulateGrad]
			1785186377056 -> 1785186374464
		1781571281920 [label="layer3.17.bn1.bias
 (64)" fillcolor=lightblue]
			1781571281920 -> 1785186377056
		1785186377056 [label=AccumulateGrad]
			1785186377248 -> 1785186373984
		1781571282816 [label="layer3.17.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
			1781571282816 -> 1785186377248
		1785186377248 [label=AccumulateGrad]
			1785186376000 -> 1785186376576
		1781571281728 [label="layer3.17.bn2.weight
 (64)" fillcolor=lightblue]
			1781571281728 -> 1785186376000
		1785186376000 [label=AccumulateGrad]
			1785186377152 -> 1785186376576
		1781571282880 [label="layer3.17.bn2.bias
 (64)" fillcolor=lightblue]
			1781571282880 -> 1785186377152
		1785186377152 [label=AccumulateGrad]
			1785186376288 -> 1785186374080
			1785186375040 -> 1785186375904
		1785186375040 [label=TBackward]
			1785186374368 -> 1785186375040
		1781674695872 [label="fc.weight
 (100, 64)" fillcolor=lightblue]
			1781674695872 -> 1785186374368
		1785186374368 [label=AccumulateGrad]
			1785186375904 -> 1781570756224
}